{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **XGBoost Algorithm**\n",
        "\n",
        "#### Introduction:\n",
        "XGBoost (Extreme Gradient Boosting) is a highly efficient, scalable, and accurate implementation of gradient boosting. It has gained widespread popularity in data science and machine learning competitions (like Kaggle) due to its excellent performance, flexibility, and optimization techniques.\n",
        "\n",
        "#### Motivation:\n",
        "XGBoost is an extension of the Gradient Boosting framework designed to optimize speed and performance by adding regularization, improving parallelization, and reducing overfitting. It builds multiple decision trees sequentially, where each new tree corrects the errors of the previous trees. \n",
        "\n",
        "### Mathematical Intuition:\n",
        "XGBoost minimizes a regularized objective function, which consists of a loss function and a regularization term.\n",
        "\n",
        "#### Objective Function:\n",
        "The objective function for XGBoost is:\n",
        "\n",
        "```python\n",
        "Obj(Θ) = L(Θ) + Ω(Θ)\n",
        "```\n",
        "Where:\n",
        "- \\( L(Θ) \\) is the loss function (measuring how well the model fits the data),\n",
        "- \\( Ω(Θ) \\) is the regularization term (to penalize complexity and prevent overfitting).\n",
        "\n",
        "For classification tasks, the typical loss function is **log loss** or **cross-entropy loss**, and for regression tasks, it can be **mean squared error (MSE)**.\n",
        "\n",
        "The regularization term is:\n",
        "\n",
        "```python\n",
        "Ω(Θ) = γT + 0.5λ∑(w_j^2)\n",
        "```\n",
        "Where:\n",
        "- \\( γ \\) controls the complexity of the tree,\n",
        "- \\( λ \\) penalizes large weights (\\( w_j \\)) to prevent overfitting,\n",
        "- \\( T \\) is the number of terminal nodes (leaves) in the tree.\n",
        "\n",
        "#### Gradient Descent in XGBoost:\n",
        "XGBoost uses **additive training**, where new trees are added to minimize the residual errors. The new model is combined with the existing one by learning a function \\( h(x) \\) that minimizes the residuals between the true value \\( y \\) and the predicted value \\( \\hat{y} \\).\n",
        "\n",
        "The algorithm minimizes the following objective:\n",
        "\n",
        "```python\n",
        "L = ∑ l(y_i, ŷ_i^(t-1) + f_t(x_i)) + Ω(f_t)\n",
        "```\n",
        "Where:\n",
        "- \\( l \\) is the loss function (such as squared error for regression),\n",
        "- \\( ŷ_i^{(t-1)} \\) is the prediction from the model at iteration \\( t-1 \\),\n",
        "- \\( f_t(x_i) \\) is the new model added at iteration \\( t \\).\n",
        "\n",
        "For the new learner \\( f_t \\), the second-order Taylor expansion of the loss function is used:\n",
        "\n",
        "```python\n",
        "Obj^(t) ≈ ∑(g_i f_t(x_i) + 0.5 h_i f_t^2(x_i)) + Ω(f_t)\n",
        "```\n",
        "Where:\n",
        "- \\( g_i \\) is the first-order gradient of the loss function w.r.t \\( ŷ_i \\),\n",
        "- \\( h_i \\) is the second-order gradient (Hessian) of the loss function w.r.t \\( ŷ_i \\).\n",
        "\n",
        "### Example of XGBoost for Classification:\n",
        "\n",
        "```python\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "### How it works:\n",
        "- **Dataset**: We create a synthetic binary classification dataset.\n",
        "- **Model**: `XGBClassifier` is used with parameters such as `n_estimators` (number of trees) and `learning_rate`.\n",
        "- **Prediction and Evaluation**: The model is trained, and accuracy is calculated on the test set.\n",
        "\n",
        "### Key Features of XGBoost:\n",
        "1. **Regularization**: L1 and L2 regularization to prevent overfitting.\n",
        "2. **Sparsity Awareness**: Handles sparse data efficiently.\n",
        "3. **Tree Pruning**: Uses a max-depth parameter and automatically prunes unnecessary branches to prevent overfitting.\n",
        "4. **Weighted Quantile Sketch**: Helps find the best split for weighted datasets.\n",
        "5. **Cross-validation**: Supports in-built cross-validation for hyperparameter tuning.\n",
        "\n",
        "### Pros of XGBoost:\n",
        "1. **Highly Efficient and Fast**: Optimized for performance and speed using techniques like tree pruning, parallelism, and handling missing values.\n",
        "2. **Great Predictive Power**: Delivers high accuracy for classification and regression tasks.\n",
        "3. **Regularization**: Built-in regularization helps prevent overfitting, improving generalization.\n",
        "4. **Scalable**: Can handle large datasets with millions of instances efficiently.\n",
        "5. **Feature Importance**: Provides insights into the most important features.\n",
        "\n",
        "### Cons of XGBoost:\n",
        "1. **Requires Careful Tuning**: To achieve optimal performance, hyperparameters like learning rate, depth, and number of trees need to be fine-tuned.\n",
        "2. **Memory Intensive**: Large datasets can lead to high memory consumption.\n",
        "3. **Long Training Time**: Though efficient, training time can still be longer compared to simpler models, especially if the dataset is very large.\n",
        "4. **Complexity**: The algorithm can be harder to interpret than simple decision trees or linear models.\n",
        "\n",
        "### Conclusion:\n",
        "XGBoost is a powerful and flexible boosting algorithm that has been widely adopted for machine learning tasks requiring high accuracy and performance. Its mathematical foundation is rooted in gradient boosting, but with enhancements like regularization, parallel processing, and improved handling of missing data. Proper hyperparameter tuning is critical to fully leverage its potential in both classification and regression tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2btI6RkCQY2"
      },
      "source": [
        "Import all the required frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2fQxl3OQBcLX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "abAkt9jMCPrN"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A9KHZdEqe1N"
      },
      "source": [
        "Load the data and splitting it into train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rd30H8xlPp7R"
      },
      "outputs": [],
      "source": [
        "# Load the California Housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X, y = california_housing.data, california_housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTNdMQW8qjdn",
        "outputId": "4cdf64c1-ed64-4309-fbef-ec507748e8e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
            "          37.88      , -122.23      ],\n",
            "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
            "          37.86      , -122.22      ],\n",
            "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
            "          37.85      , -122.24      ],\n",
            "       ...,\n",
            "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
            "          39.43      , -121.22      ],\n",
            "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
            "          39.43      , -121.32      ],\n",
            "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
            "          39.37      , -121.24      ]]), 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]), 'frame': None, 'target_names': ['MedHouseVal'], 'feature_names': ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude'], 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 20640\\n\\n:Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n:Attribute Information:\\n    - MedInc        median income in block group\\n    - HouseAge      median house age in block group\\n    - AveRooms      average number of rooms per household\\n    - AveBedrms     average number of bedrooms per household\\n    - Population    block group population\\n    - AveOccup      average number of household members\\n    - Latitude      block group latitude\\n    - Longitude     block group longitude\\n\\n:Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. rubric:: References\\n\\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n  Statistics and Probability Letters, 33 (1997) 291-297\\n'}\n"
          ]
        }
      ],
      "source": [
        "print(california_housing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "RfxnDjQuqmO4",
        "outputId": "1f9138d9-eda4-4e55-f9c3-3abb43c576d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nparam_grid_xgb = {\\n    \\'lambda\\': [0.01, 0.1, 1, 10],\\n    \\'gamma\\': [0, 0.1, 1, 10],\\n    \\'max_depth\\': [3, 5, 7],\\n    \\'learning_rate\\': [0.01, 0.1, 0.2],\\n    \\'n_estimators\\': [100, 200, 300]\\n}\\n\\nparam_grid_rf = {\\n    \\'n_estimators\\': [100, 200, 300],\\n    \\'max_depth\\': [None, 10, 20, 30],\\n    \\'min_samples_split\\': [2, 5, 10],\\n    \\'min_samples_leaf\\': [1, 2, 4]\\n}\\n\\nparam_grid_gb = {\\n    \\'n_estimators\\': [100, 200, 300],\\n    \\'learning_rate\\': [0.01, 0.1, 0.2],\\n    \\'max_depth\\': [3, 5, 7],\\n    \\'min_samples_split\\': [2, 5, 10],\\n    \\'min_samples_leaf\\': [1, 2, 4]\\n}\\n\\n# Set up GridSearchCV for each model\\ngrid_search_xgb = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid_xgb, cv=5, scoring=\\'neg_mean_squared_error\\', verbose=1, n_jobs=-1)\\ngrid_search_rf = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf, cv=5, scoring=\\'neg_mean_squared_error\\', verbose=1, n_jobs=-1)\\ngrid_search_gb = GridSearchCV(estimator=gb_regressor, param_grid=param_grid_gb, cv=5, scoring=\\'neg_mean_squared_error\\', verbose=1, n_jobs=-1)\\n\\n# Fit the models\\nprint(\"Tuning XGBoost...\")\\ngrid_search_xgb.fit(X_train, y_train)\\nprint(\"Tuning RandomForest...\")\\ngrid_search_rf.fit(X_train, y_train)\\nprint(\"Tuning GradientBoosting...\")\\ngrid_search_gb.fit(X_train, y_train)\\n\\n# Best models\\nbest_xgb = grid_search_xgb.best_estimator_\\nbest_rf = grid_search_rf.best_estimator_\\nbest_gb = grid_search_gb.best_estimator_\\n\\n\\n# Predictions\\ny_pred_xgb = best_xgb.predict(X_test)\\ny_pred_rf = best_rf.predict(X_test)\\ny_pred_gb = best_gb.predict(X_test)\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the models\n",
        "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Data Modeling\n",
        "model_xgb = xgb_regressor.fit(X_train, y_train)\n",
        "model_rf = rf_regressor.fit(X_train, y_train)\n",
        "model_gb = gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Define parameter grids for each model\n",
        "\"\"\"\n",
        "param_grid_xgb = {\n",
        "    'lambda': [0.01, 0.1, 1, 10],\n",
        "    'gamma': [0, 0.1, 1, 10],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 300]\n",
        "}\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV for each model\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid_xgb, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
        "grid_search_rf = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
        "grid_search_gb = GridSearchCV(estimator=gb_regressor, param_grid=param_grid_gb, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the models\n",
        "print(\"Tuning XGBoost...\")\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "print(\"Tuning RandomForest...\")\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "print(\"Tuning GradientBoosting...\")\n",
        "grid_search_gb.fit(X_train, y_train)\n",
        "\n",
        "# Best models\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "best_gb = grid_search_gb.best_estimator_\n",
        "\n",
        "\n",
        "# Predictions\n",
        "y_pred_xgb = best_xgb.predict(X_test)\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "y_pred_gb = best_gb.predict(X_test)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GstxR7ANremf"
      },
      "source": [
        "Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9_CtVPj8rbr-"
      },
      "outputs": [],
      "source": [
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "y_pred_rf = model_rf.predict(X_test)\n",
        "y_pred_gb = model_gb.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH67hIJLrmlw"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q8LeVixrlfH",
        "outputId": "cf60bb0e-c2d0-4d0d-cabe-0d9634573315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error for XGBRegressor:  0.2225899267544737\n",
            "Mean Squared Error for RandomForestRegressor:  0.2557259876588585\n",
            "Mean Squared Error for GradientBoostingRegressor:  0.2939973248643864\n"
          ]
        }
      ],
      "source": [
        "# Calculate Mean Squared Error\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "\n",
        "# Print the results\n",
        "#print(\"Best parameters for XGBRegressor: \", grid_search_xgb.best_params_)\n",
        "print(\"Mean Squared Error for XGBRegressor: \", mse_xgb)\n",
        "\n",
        "#print(\"Best parameters for RandomForestRegressor: \", grid_search_rf.best_params_)\n",
        "print(\"Mean Squared Error for RandomForestRegressor: \", mse_rf)\n",
        "\n",
        "#print(\"Best parameters for GradientBoostingRegressor: \", grid_search_gb.best_params_)\n",
        "print(\"Mean Squared Error for GradientBoostingRegressor: \", mse_gb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
