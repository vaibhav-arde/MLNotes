{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression related terms\n",
    "\n",
    "**Note:** Linear regression is a foundational algorithm in machine learning, providing a solid understanding of the core concepts. It's essential to grasp these terms to delve deeper into more complex models and techniques.\n",
    "\n",
    "### Linear Regression\n",
    "* **Definition:** A statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (predictors) by fitting a linear equation to the observed data.\n",
    "* **Goal:** To find the best-fitting line that minimizes the difference between the predicted values and the actual values.\n",
    "\n",
    "### Gradient Descent\n",
    "* **Definition:** An optimization algorithm used to find the minimum of a function. In machine learning, it's used to find the optimal parameters (weights and biases) of a model.\n",
    "* **Process:** It iteratively adjusts the parameters in the direction of steepest descent (negative gradient) of the error function until a minimum is reached.\n",
    "\n",
    "### Gradient Descent Optimizer\n",
    "* **Definition:** An algorithm that implements the gradient descent process.\n",
    "* **Role:** It determines how the parameters are updated at each iteration.\n",
    "* **Examples:** Stochastic Gradient Descent (SGD), Adam, RMSprop.\n",
    "\n",
    "### Best Fit Line: y = mx + c\n",
    "* **Definition:** The straight line that best represents the relationship between two variables on a scatter plot.\n",
    "* **Equation:** y = mx + c, where:\n",
    "    * y is the dependent variable\n",
    "    * x is the independent variable\n",
    "    * m is the slope of the line\n",
    "    * c is the y-intercept (the value of y when x is 0)\n",
    "\n",
    "### Slope (m)\n",
    "* **Definition:** The rate of change of the dependent variable with respect to the independent variable. It represents the steepness of the line.\n",
    "* **Interpretation:** A positive slope indicates a positive relationship between the variables, while a negative slope indicates a negative relationship.\n",
    "\n",
    "### Intercept (c)\n",
    "* **Definition:** The value of the dependent variable when the independent variable is zero. It's the point where the line crosses the y-axis.\n",
    "\n",
    "### Error (Residual)\n",
    "* **Definition:** The difference between the actual value of the dependent variable and the predicted value from the regression line.\n",
    "* **Role:** The goal of linear regression is to minimize the sum of squared errors.\n",
    "\n",
    "### Global Minima\n",
    "* **Definition:** The lowest point of a function over its entire domain.\n",
    "* **Goal:** In gradient descent, the aim is to find the global minimum of the error function to achieve the best model performance.\n",
    "\n",
    "### Mathematical Intuition of Linear Regression\n",
    "* **Objective:** To find the values of m and c that minimize the sum of squared errors between the observed data points and the predicted values on the line.\n",
    "* **Method:**\n",
    "    1. Initialize random values for m and c.\n",
    "    2. Calculate the error for each data point.\n",
    "    3. Calculate the gradient of the error function with respect to m and c.\n",
    "    4. Update m and c using gradient descent.\n",
    "    5. Repeat steps 2-4 until the error converges to a minimum.\n",
    "* **Underlying principle:** The line that minimizes the sum of squared errors is the best fit line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
