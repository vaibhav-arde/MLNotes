{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decision Tree: Overview**\n",
    "\n",
    "A **Decision Tree** is a supervised learning algorithm used for both classification and regression tasks. It works by recursively splitting the dataset into subsets based on the value of input features. The splits are chosen in such a way that they create the most homogeneous groups with respect to the target variable.\n",
    "\n",
    "A decision tree resembles a tree structure with nodes representing different decision points, and the branches representing outcomes. The tree is constructed from root to leaves, where:\n",
    "- **Root Node**: Represents the entire dataset and the best split.\n",
    "- **Internal/Decision Nodes**: Represent a test on an attribute and possible outcomes (branches).\n",
    "- **Leaf/Terminal Nodes**: Represent the final prediction or decision (class label or value).\n",
    "\n",
    "---\n",
    "\n",
    "### **Components of a Decision Tree**\n",
    "\n",
    "#### **1. Root Node**\n",
    "- The top node of a decision tree.\n",
    "- Represents the entire dataset before any splits.\n",
    "- The split at this point results in the maximum reduction in impurity (e.g., Gini, entropy).\n",
    "\n",
    "#### **2. Decision Nodes**\n",
    "- Internal nodes that represent the feature upon which the dataset is split.\n",
    "- Each internal node splits the dataset based on a feature, and the decision made determines which branch to follow.\n",
    "\n",
    "#### **3. Leaf Nodes (Terminal Nodes)**\n",
    "- Nodes that do not split further.\n",
    "- Each leaf represents a class label (in classification) or a value (in regression).\n",
    "- All the data points reaching a particular leaf node belong to the same class (or approximate value in regression).\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Nodes**\n",
    "\n",
    "1. **Root Node**: The top-most node in a decision tree, representing the entire dataset before any split.\n",
    "2. **Internal/Decision Node**: A node representing a decision point where the data is split based on a feature.\n",
    "3. **Leaf Node**: The end node that provides a classification or regression outcome.\n",
    "\n",
    "---\n",
    "\n",
    "### **Impurity in Decision Trees**\n",
    "\n",
    "Impurity refers to the degree of disorder or randomness in a dataset. When splitting nodes, decision trees aim to reduce impurity to create homogeneous branches (subsets). Different metrics can be used to measure impurity:\n",
    "\n",
    "#### **1. Gini Impurity**\n",
    "\n",
    "The **Gini Impurity** measures how often a randomly chosen element from the set would be incorrectly classified if it were randomly classified according to the distribution of class labels in the set.\n",
    "\n",
    "- **Formula**:\n",
    "```python\n",
    "Gini = 1 - Σ(p_i^2)\n",
    "```\n",
    "Where `p_i` is the probability of class `i`.\n",
    "\n",
    "- **Range**: The value of Gini impurity ranges from 0 (pure node, all elements are of the same class) to 0.5 (impure node, equal distribution of classes).\n",
    "\n",
    "#### **2. Entropy (Information Gain)**\n",
    "\n",
    "**Entropy** is a measure from information theory that quantifies the amount of uncertainty or impurity in the data. The goal is to reduce entropy as the tree grows.\n",
    "\n",
    "- **Formula**:\n",
    "```python\n",
    "Entropy = - Σ(p_i * log2(p_i))\n",
    "```\n",
    "Where `p_i` is the probability of class `i`.\n",
    "\n",
    "- **Range**: Entropy values range from 0 (pure node) to 1 (maximum impurity for binary classification).\n",
    "\n",
    "- **Information Gain**: It represents the reduction in entropy after the dataset is split on an attribute.\n",
    "  - **Formula**:\n",
    "```python\n",
    "Information Gain = Entropy(parent) - Weighted Sum of Entropy(children)\n",
    "```\n",
    "\n",
    "#### **3. Mean Squared Error (MSE)**\n",
    "\n",
    "For **regression trees**, the impurity measure is typically **Mean Squared Error (MSE)**. The goal is to minimize the variance within each node, which is equivalent to reducing the MSE.\n",
    "\n",
    "- **Formula**:\n",
    "```python\n",
    "MSE = (1/N) * Σ(y_i - ŷ)^2\n",
    "```\n",
    "Where `y_i` is the actual value, and `ŷ` is the predicted value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Splitting Criteria**\n",
    "\n",
    "When constructing a decision tree, the algorithm evaluates different features and thresholds to determine the best split. This is done by calculating the **impurity** (Gini, entropy, or MSE) for each possible split and selecting the one that reduces impurity the most. The process continues recursively until a stopping criterion is met (e.g., maximum depth of the tree, minimum number of samples in a node, etc.).\n",
    "\n",
    "1. **Gini Impurity** is typically used in **CART (Classification and Regression Trees)**.\n",
    "2. **Entropy/Information Gain** is often used in **ID3 and C4.5** decision tree algorithms.\n",
    "3. **MSE** is used for **regression trees** to measure the variance in the data at each split.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of a Decision Tree**\n",
    "\n",
    "Consider the task of predicting whether a person will buy a car based on income and age:\n",
    "\n",
    "1. **Root Node**: The dataset is split based on the feature `income`. People with income > `X` go one way, others go another way.\n",
    "2. **Decision Nodes**: The `age` feature is used to further split the data into younger and older individuals.\n",
    "3. **Leaf Nodes**: After splitting based on income and age, the final leaf nodes represent predictions such as \"Buys car\" or \"Does not buy car.\"\n",
    "\n",
    "Each split tries to make the nodes purer by reducing the impurity (Gini, Entropy, or MSE).\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "- **Decision Trees** are interpretable, easy to visualize, and versatile in handling both classification and regression problems.\n",
    "- **Impurity** measures like **Gini** and **Entropy** guide the splitting process to create homogeneous nodes.\n",
    "- **Leaf Nodes** provide the final predictions, and the **decision nodes** make logical choices that direct the flow of the tree.\n",
    "\n",
    "The tree structure makes it easy to understand how decisions are made and how the model arrives at its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gini Index**\n",
    "\n",
    "The **Gini Index** (or **Gini Impurity**) is a metric used in decision trees to measure the impurity of a node. It represents the probability that a randomly chosen element from the dataset would be incorrectly classified if it were randomly labeled according to the class distribution at that node.\n",
    "\n",
    "The goal of a decision tree algorithm is to split the nodes in such a way that the resulting nodes have lower impurity. The **Gini Index** helps quantify the impurity before and after a split, allowing the algorithm to choose the best possible split.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula for Gini Index**\n",
    "\n",
    "For a node with `k` possible classes (categories), the Gini Index is defined as:\n",
    "\n",
    "``` \n",
    "Gini = 1 - Σ(p_i^2)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `p_i` is the proportion of instances that belong to class `i` at a particular node.\n",
    "\n",
    "### **Interpretation of Gini Index**\n",
    "\n",
    "- **Gini = 0**: This means the node is **pure**; all elements belong to a single class.\n",
    "- **Gini > 0**: The node contains elements from multiple classes, indicating impurity.\n",
    "- **Maximum Gini (e.g., Gini = 0.5 for binary classification)**: This occurs when classes are evenly distributed, meaning the node is highly impure.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation of Gini Index**\n",
    "\n",
    "Imagine a node where the dataset has two classes (binary classification), `Class 0` and `Class 1`.\n",
    "\n",
    "#### **Example 1: Pure Node**\n",
    "Suppose the node contains 10 instances, all of which belong to `Class 0`. The proportions are:\n",
    "- `p_0 = 1.0` (100% belong to Class 0)\n",
    "- `p_1 = 0.0` (0% belong to Class 1)\n",
    "\n",
    "The Gini Index would be:\n",
    "\n",
    "``` \n",
    "Gini = 1 - (1.0^2 + 0.0^2) \n",
    "     = 1 - 1.0 \n",
    "     = 0\n",
    "```\n",
    "This indicates a **pure node** (all elements belong to one class).\n",
    "\n",
    "#### **Example 2: Impure Node**\n",
    "Now, suppose the node contains 10 instances, with 4 instances of `Class 0` and 6 instances of `Class 1`. The proportions are:\n",
    "- `p_0 = 4/10 = 0.4`\n",
    "- `p_1 = 6/10 = 0.6`\n",
    "\n",
    "The Gini Index would be:\n",
    "\n",
    "``` \n",
    "Gini = 1 - (0.4^2 + 0.6^2) \n",
    "     = 1 - (0.16 + 0.36) \n",
    "     = 1 - 0.52 \n",
    "     = 0.48\n",
    "```\n",
    "This indicates some **impurity** in the node since it contains elements from both classes.\n",
    "\n",
    "#### **Example 3: Completely Impure Node**\n",
    "Suppose the node contains 10 instances evenly distributed between `Class 0` and `Class 1` (5 instances each). The proportions are:\n",
    "- `p_0 = 5/10 = 0.5`\n",
    "- `p_1 = 5/10 = 0.5`\n",
    "\n",
    "The Gini Index would be:\n",
    "\n",
    "``` \n",
    "Gini = 1 - (0.5^2 + 0.5^2) \n",
    "     = 1 - (0.25 + 0.25) \n",
    "     = 1 - 0.5 \n",
    "     = 0.5\n",
    "```\n",
    "This indicates **maximum impurity** for a binary classification problem.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gini Index in Decision Trees**\n",
    "\n",
    "In decision trees (e.g., **CART** - Classification and Regression Trees), the Gini Index is used as a criterion to determine the best split. The algorithm chooses the feature and threshold that results in the **lowest weighted average Gini Index** for the child nodes after the split.\n",
    "\n",
    "1. **Pre-Split Gini Index**: The Gini Index of the node before splitting.\n",
    "2. **Post-Split Gini Index**: The Gini Index of the child nodes after the split. The algorithm calculates the weighted average of these values based on the size of the child nodes.\n",
    "\n",
    "The split that results in the greatest reduction in Gini Index (the highest reduction in impurity) is selected.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Gini Index**\n",
    "\n",
    "- **Computationally efficient**: Gini Index is faster to compute than entropy, which involves logarithmic calculations.\n",
    "- **Effective for Classification**: It performs well in decision tree algorithms for classification tasks, creating splits that help separate the classes effectively.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "The **Gini Index** is a measure of node impurity used to guide the decision-making process in decision trees. By minimizing Gini, the tree becomes better at creating pure branches, which improve the model's predictive power. The Gini Index is popular in the CART algorithm because of its simplicity and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy** is a fundamental concept in information theory and machine learning, often used in decision trees to determine the purity of a node. It measures the level of uncertainty or impurity in a dataset. \n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Entropy Formula:**  \n",
    "   Entropy is calculated using the formula:\n",
    "\n",
    "H(S)= − ∑ p_i log_2 p_i\n",
    "​\n",
    "\n",
    "   Where:\n",
    "   - \\(H(S)\\) is the entropy of the dataset \\(S\\).\n",
    "   - \\(p_i\\) is the probability of class \\(i\\) in the dataset.\n",
    "   - \\(n\\) is the total number of classes.\n",
    "\n",
    "- **Interpretation:**\n",
    "  - If **all elements** in a dataset belong to the **same class**, the entropy is **0** (i.e., no uncertainty or impurity).\n",
    "  - If the dataset contains a **50-50 split** between two classes, the entropy is **1** (i.e., maximum uncertainty).\n",
    "  - Entropy is highest when the classes are equally distributed.\n",
    "\n",
    "### Entropy in Decision Trees:\n",
    "In decision trees, entropy helps to choose the **best attribute** for splitting the data. The goal is to select an attribute that **minimizes entropy** after the split, which is referred to as **Information Gain**. A lower entropy means higher homogeneity within the subset after the split, leading to better decision-making.\n",
    "\n",
    "### Example:\n",
    "Consider a dataset where we want to classify whether it is **sunny** or **rainy** based on past weather data. If 80% of the data is labeled as **sunny** and 20% as **rainy**, the entropy would be lower than a dataset where there is a 50-50 distribution, indicating that the first dataset is more pure or predictable.\n",
    "\n",
    "Entropy plays a crucial role in **classification algorithms** like decision trees and helps optimize their accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Entropy\n",
    "\n",
    "**Entropy** is a measure of the uncertainty or disorder in a dataset. In the context of decision trees, it quantifies the impurity of a node.\n",
    "\n",
    "### Key Points:\n",
    "* **Higher Entropy:** A higher entropy indicates a more mixed or uncertain dataset, where the classes are distributed more evenly.\n",
    "* **Lower Entropy:** A lower entropy indicates a more pure dataset, where one or a few classes dominate.\n",
    "* **Maximum Entropy:** The maximum entropy occurs when the classes are perfectly balanced (e.g., 50-50 split).\n",
    "* **Minimum Entropy:** The minimum entropy occurs when all instances belong to the same class.\n",
    "\n",
    "### Formula:\n",
    "```\n",
    "Entropy(S) = -∑(p_i * log2(p_i))\n",
    "```\n",
    "where:\n",
    "- `S` is the dataset or node\n",
    "- `p_i` is the proportion of instances belonging to class `i`\n",
    "\n",
    "## Information Gain\n",
    "\n",
    "**Information Gain (IG)** measures the reduction in entropy achieved by splitting a dataset on a particular attribute. It quantifies how much the attribute helps to separate the classes.\n",
    "\n",
    "### Key Points:\n",
    "* **Higher Information Gain:** A higher information gain indicates that the attribute is more informative for classification, as it effectively reduces the impurity of the dataset.\n",
    "* **Lower Information Gain:** A lower information gain suggests that the attribute is less informative, as it doesn't significantly reduce the impurity.\n",
    "\n",
    "### Formula:\n",
    "```\n",
    "Information Gain(S, A) = Entropy(S) - ∑((|S_v|/|S|) * Entropy(S_v))\n",
    "```\n",
    "where:\n",
    "- `S` is the parent node\n",
    "- `A` is the attribute\n",
    "- `S_v` is the subset of `S` that has attribute `A` with value `v`\n",
    "- `|S_v|` is the number of instances in `S_v`\n",
    "- `|S|` is the total number of instances in `S`\n",
    "\n",
    "## Decision Tree Building\n",
    "\n",
    "1. **Calculate Entropy:** Calculate the entropy of the root node.\n",
    "2. **Calculate Information Gain:** For each attribute, calculate the information gain.\n",
    "3. **Choose Best Attribute:** Select the attribute with the highest information gain.\n",
    "4. **Split:** Split the node based on the chosen attribute.\n",
    "5. **Repeat:** Recursively apply steps 2-4 to each child node until a stopping criterion is met.\n",
    "\n",
    "**In summary,** entropy measures the impurity of a dataset, while information gain measures the improvement in purity achieved by splitting the data based on a particular attribute. Decision trees use these concepts to construct a model that accurately predicts the target variable by selecting the most informative features for splitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gini vs. Entropy** in Decision Trees\n",
    "\n",
    "Both **Gini Index** and **Entropy** are metrics used to measure the **impurity** or **disorder** of a dataset in **Decision Trees**. They help determine the best feature to split the data on. However, there are some key differences between them.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Between Gini and Entropy**\n",
    "\n",
    "| **Aspect**              | **Gini Index**                               | **Entropy**                                 |\n",
    "|-------------------------|----------------------------------------------|--------------------------------------------|\n",
    "| **Range**               | 0 (pure) to 0.5 (binary classes equally split)| 0 (pure) to 1 (binary classes equally split)|\n",
    "| **Formula Simplicity**  | Simpler and faster to compute                | More complex due to logarithmic calculations|\n",
    "| **Speed**               | Computationally faster                       | Computationally slower due to logarithms    |\n",
    "| **Preference**          | Tends to favor larger partitions             | More sensitive to outliers and smaller partitions|\n",
    "| **Usage**               | Used by **CART (Classification and Regression Trees)** | Used by **ID3** and **C4.5 Decision Trees**|\n",
    "| **Performance**         | Often leads to similar performance as entropy, but slightly more efficient | Slightly slower due to log calculations, but sometimes preferred for nuanced splits |\n",
    "\n",
    "### **Practical Differences:**\n",
    "- **Gini** is computationally faster and easier to implement, making it a preferred choice for **CART (Classification and Regression Trees)**. It usually leads to similar results as entropy but may favor larger splits.\n",
    "- **Entropy** tends to be more sensitive to smaller groups or rare classes, so it sometimes makes more nuanced decisions in splits. It is used in algorithms like **ID3** and **C4.5**.\n",
    "\n",
    "### **Which to Use?**\n",
    "- **Gini** is preferred for computational efficiency, especially when working with large datasets.\n",
    "- **Entropy** might be chosen when more detailed splits and better handling of imbalanced classes are required.\n",
    "\n",
    "In most cases, both metrics will lead to similar decision trees with slightly different splitting points, and the choice depends on the specific problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gini vs. Entropy: Highest Purity and Impurity**\n",
    "\n",
    "Both **Gini Index** and **Entropy** measure the purity (or impurity) of a dataset, particularly for use in decision tree algorithms. Here's a breakdown of how each behaves in terms of **highest purity** and **highest impurity**:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Gini Index:**\n",
    "- **Range:** Gini Index values range between **0** and **0.5** for binary classification.\n",
    "  \n",
    "#### **Highest Purity:**\n",
    "- **Gini Index = 0:**  \n",
    "  - Occurs when the dataset is **pure** (all instances belong to a single class).\n",
    "  - Example: If all instances in a dataset belong to class A (100% A, 0% B), the Gini Index will be 0.\n",
    "\n",
    "#### **Highest Impurity:**\n",
    "- **Gini Index = 0.5:**  \n",
    "  - Occurs when the dataset is **maximally impure** (the classes are evenly split).\n",
    "  - Example: If 50% of the instances belong to class A and 50% to class B, the Gini Index will be 0.5, representing maximum uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Entropy:**\n",
    "- **Range:** Entropy values range between **0** and **1** for binary classification.\n",
    "\n",
    "#### **Highest Purity:**\n",
    "- **Entropy = 0:**  \n",
    "  - Occurs when the dataset is **pure** (all instances belong to a single class).\n",
    "  - Example: If all instances in a dataset belong to class A (100% A, 0% B), the entropy will be 0.\n",
    "\n",
    "#### **Highest Impurity:**\n",
    "- **Entropy = 1:**  \n",
    "  - Occurs when the dataset is **maximally impure** (the classes are evenly split).\n",
    "  - Example: If 50% of the instances belong to class A and 50% to class B, the entropy will be 1, representing maximum disorder or uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Values for Binary Classification:**\n",
    "\n",
    "| **Measure**  | **Pure State (100% of one class)** | **Maximally Impure State (50-50 split)** |\n",
    "|--------------|-----------------------------------|------------------------------------------|\n",
    "| **Gini Index** | 0                                  | 0.5                                      |\n",
    "| **Entropy**   | 0                                  | 1                                        |\n",
    "\n",
    "### **Takeaways:**\n",
    "- **Pure State (0 Impurity):** Both Gini and Entropy are **0** when the dataset is pure (all instances belong to a single class).\n",
    "- **Maximally Impure State:**  \n",
    "  - For **Gini Index**, the maximum impurity is **0.5** for a 50-50 class split.  \n",
    "  - For **Entropy**, the maximum impurity is **1** for a 50-50 class split.\n",
    "\n",
    "Both metrics tend to favor splits that lead to purer subsets of data, but they differ slightly in scale and sensitivity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
