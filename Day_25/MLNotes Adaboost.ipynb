{"cells":[{"cell_type":"markdown","metadata":{},"source":["# AdaBoost (Adaptive Boosting)\n","\n","AdaBoost (Adaptive Boosting) is a powerful ensemble learning technique that combines multiple weak learners to create a strong classifier. It iteratively adjusts the weights of misclassified examples to focus more on difficult cases, improving performance.\n","\n","## Steps in AdaBoost\n","\n","1. **Initialize Weights**:  \n","   Every sample in the training data is assigned equal weight.  \n","   - Formula: `w_i = \\frac{1}{N}` , where \\( N \\) is the total number of samples.\n","\n","2. **Train Weak Learner**:  \n","   A weak learner, like a decision stump, is trained on the weighted data, and its classification error is calculated.  \n","   - Formula:  \n","   ` \\epsilon = \\sum_{i=1}^{N} w_i \\times I(y_i \\neq h(x_i)) `,  \n","   where \\( I \\) is the indicator function (1 for misclassified samples, 0 otherwise).\n","\n","3. **Calculate Weak Learner's Weight**:  \n","   The weight (alpha) of the weak learner is calculated based on its error \\( \\epsilon \\).  \n","   - Formula:  \n","   ` \\alpha = \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon}{\\epsilon}\\right) `.\n","\n","4. **Update Weights**:  \n","   Increase the weights of misclassified samples, so that the next learner focuses on them.  \n","   - Formula:  \n","   ` w_i = w_i \\times \\exp(\\alpha \\times I(y_i \\neq h(x_i))) `.  \n","   Normalize weights to ensure the sum is 1.\n","\n","5. **Train Next Learner**:  \n","   Repeat steps 2 to 4 for the next weak learner, which now focuses on the updated weights.\n","\n","6. **Final Prediction**:  \n","   Combine the weak learners using a weighted vote to make the final prediction.  \n","   - Formula:  \n","   ` H(x) = \\text{sign} \\left( \\sum_{m=1}^{M} \\alpha_m h_m(x) \\right) `,  \n","   where \\( \\alpha_m \\) is the weight of the \\( m \\)-th weak learner, and \\( h_m(x) \\) is the prediction of the \\( m \\)-th weak learner.\n","\n","## Example\n","\n","Let's assume a binary classification problem with 3 data points, initialized with equal weights:\n","\n","1. A weak learner misclassifies one data point, yielding an error \\( \\epsilon \\).\n","2. The weak learner's weight is calculated using the error.\n","3. The sample weights are updated, assigning higher weight to the misclassified point.\n","4. The next weak learner focuses on the misclassified point.\n","5. This process continues, and in the end, all learners are combined for the final prediction.\n","\n","## Real-World Applications\n","\n","- **Face Detection**: AdaBoost is used in the Viola-Jones algorithm for detecting faces in images.\n","- **Fraud Detection**: Helps identify fraudulent transactions.\n","- **Customer Churn Prediction**: Classifies customers likely to churn based on historical data.\n","\n","## Pros\n","\n","- Can **improve accuracy** by focusing on difficult cases.\n","- Works well with **imbalanced data**.\n","- **No parameter tuning** for weak learners.\n","\n","## Cons\n","\n","- **Sensitive to noisy data** as it focuses too much on misclassified points.\n","- Computationally **intensive for large datasets**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
