{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Naive Bayes Classifier**\n",
    "\n",
    "### **Bayes' Theorem**\n",
    "\n",
    "Bayes' Theorem is a fundamental concept in probability theory and statistics that describes the likelihood of an event based on prior knowledge of conditions related to the event. It provides a way to update the probability of a hypothesis (event) given new evidence.\n",
    "\n",
    "The formula for **Bayes' Theorem** is:\n",
    "\n",
    "```python\n",
    "P(A|B) = [P(B|A) * P(A)] / P(B)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `P(A|B)` is the **posterior probability**: the probability of event A occurring given that B is true.\n",
    "- `P(B|A)` is the **likelihood**: the probability of event B occurring given that A is true.\n",
    "- `P(A)` is the **prior probability**: the initial probability of event A before considering B.\n",
    "- `P(B)` is the **evidence**: the probability of event B.\n",
    "\n",
    "### **Naive Bayes Classifier**\n",
    "\n",
    "The **Naive Bayes classifier** is a probabilistic machine learning model based on Bayes' Theorem. It's called \"naive\" because it assumes that the features are conditionally independent of each other given the class label, which rarely holds in real-life situations. Despite this naive assumption, Naive Bayes works surprisingly well in many complex real-world tasks, especially in text classification, spam filtering, and sentiment analysis.\n",
    "\n",
    "#### **Bayes' Theorem in Naive Bayes Classification**\n",
    "In a classification problem, Bayes' Theorem is used to compute the probability of a given class `C` given a set of features `X = (x1, x2, ..., xn)`:\n",
    "\n",
    "```python\n",
    "P(C|X) = [P(X|C) * P(C)] / P(X)\n",
    "```\n",
    "\n",
    "Since `P(X)` is the same for all classes and does not affect the classification, we focus on maximizing the numerator:\n",
    "\n",
    "```python\n",
    "P(C|X) ∝ P(C) * P(X|C)\n",
    "```\n",
    "\n",
    "Now, under the **Naive assumption** (i.e., all features are conditionally independent):\n",
    "\n",
    "```python\n",
    "P(X|C) = P(x1|C) * P(x2|C) * ... * P(xn|C)\n",
    "```\n",
    "\n",
    "Thus, the classifier predicts the class that maximizes:\n",
    "\n",
    "```python\n",
    "P(C) * P(x1|C) * P(x2|C) * ... * P(xn|C)\n",
    "```\n",
    "\n",
    "#### **Steps in Naive Bayes Classification:**\n",
    "1. **Calculate Prior Probabilities (`P(C)`)**: This is the proportion of each class in the training dataset.\n",
    "2. **Calculate Likelihood (`P(xi|C)`)**: This is the likelihood of each feature given each class.\n",
    "3. **Predict Class**: For a new example, calculate the posterior probability for each class, then predict the class with the highest probability.\n",
    "\n",
    "### **Assumptions of Naive Bayes**\n",
    "1. **Conditional Independence**: The key assumption is that the features are independent of each other given the class label. This is often not true in practice, but Naive Bayes performs well despite this limitation.\n",
    "2. **Feature Contribution**: All features contribute equally to the outcome.\n",
    "\n",
    "### **Types of Naive Bayes Classifiers**\n",
    "\n",
    "1. **Gaussian Naive Bayes**:\n",
    "   - Assumes that the continuous values associated with each feature follow a Gaussian (normal) distribution.\n",
    "   - Used for continuous data.\n",
    "   - **Example**: Predicting if a person will purchase a product based on age and salary.\n",
    "\n",
    "   **Formula:**\n",
    "   ```python\n",
    "   P(x|C) = (1 / sqrt(2 * π * σ^2)) * exp(-(x - μ)^2 / (2 * σ^2))\n",
    "   ```\n",
    "   Where `μ` is the mean and `σ` is the standard deviation of the feature.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Used for discrete features like word counts in text classification (e.g., spam detection, sentiment analysis).\n",
    "   - **Example**: Classifying an email as spam or not spam based on the frequency of certain words.\n",
    "\n",
    "3. **Bernoulli Naive Bayes**:\n",
    "   - Used for binary or boolean features.\n",
    "   - Assumes that features are binary (e.g., presence or absence of a word).\n",
    "   - **Example**: Sentiment analysis with binary word presence (whether a word appears in the document or not).\n",
    "\n",
    "### **Example of Naive Bayes Classifier**\n",
    "\n",
    "Let’s consider a **spam classification** example using a **Multinomial Naive Bayes classifier**. Assume you have a dataset with emails labeled as either \"spam\" or \"not spam\" and a set of words that appear in these emails.\n",
    "\n",
    "1. **Prior Probability (`P(C)`):**\n",
    "   - Suppose 30% of the emails in the dataset are labeled as \"spam\" and 70% as \"not spam\".\n",
    "   - `P(spam) = 0.30`\n",
    "   - `P(not spam) = 0.70`\n",
    "\n",
    "2. **Likelihood (`P(X|C)`):**\n",
    "   - Assume you have two words: \"buy\" and \"free\". The likelihood of these words given the class is calculated from the training data.\n",
    "   - `P(buy|spam) = 0.4`, `P(free|spam) = 0.8`\n",
    "   - `P(buy|not spam) = 0.1`, `P(free|not spam) = 0.05`\n",
    "\n",
    "3. **Prediction:**\n",
    "   - Given a new email containing the words \"buy\" and \"free\", we calculate the posterior probabilities:\n",
    "     - `P(spam|buy, free) ∝ P(spam) * P(buy|spam) * P(free|spam)`\n",
    "     - `P(not spam|buy, free) ∝ P(not spam) * P(buy|not spam) * P(free|not spam)`\n",
    "\n",
    "   After calculating both probabilities, the class with the higher probability will be the predicted label for the email.\n",
    "\n",
    "### **Advantages of Naive Bayes**\n",
    "\n",
    "- **Fast and Efficient**: It is computationally efficient and works well with large datasets.\n",
    "- **Performs Well with Small Data**: It doesn’t require large training datasets to perform well.\n",
    "- **Performs Well with High-Dimensional Data**: Particularly useful for text classification tasks, which often involve many features (words).\n",
    "\n",
    "### **Disadvantages of Naive Bayes**\n",
    "\n",
    "- **Independence Assumption**: The assumption that features are conditionally independent is often unrealistic in many practical scenarios. While Naive Bayes performs well despite this, it can be suboptimal when features are highly correlated.\n",
    "- **Zero Frequency Problem**: If a categorical feature in the test data has a value that was not observed in the training data, the model assigns zero probability to that event. This issue is typically handled by techniques like **Laplace Smoothing**.\n",
    "\n",
    "### **Conclusion**\n",
    "Naive Bayes is an efficient, interpretable, and widely used classification algorithm, particularly for text classification tasks. Its simplicity, coupled with surprisingly good performance despite its naive assumptions, makes it a powerful tool in many situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
