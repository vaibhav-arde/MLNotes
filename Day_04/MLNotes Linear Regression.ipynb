{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression related terms\n",
    "\n",
    "**Note:** Linear regression is a foundational algorithm in machine learning, providing a solid understanding of the core concepts. It's essential to grasp these terms to delve deeper into more complex models and techniques.\n",
    "\n",
    "### Linear Regression\n",
    "* **Definition:** A statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (predictors) by fitting a linear equation to the observed data.\n",
    "* **Goal:** To find the best-fitting line that minimizes the difference between the predicted values and the actual values.\n",
    "\n",
    "### Gradient Descent\n",
    "* **Definition:** An optimization algorithm used to find the minimum of a function. In machine learning, it's used to find the optimal parameters (weights and biases) of a model.\n",
    "* **Process:** It iteratively adjusts the parameters in the direction of steepest descent (negative gradient) of the error function until a minimum is reached.\n",
    "\n",
    "### Gradient Descent Optimizer\n",
    "* **Definition:** An algorithm that implements the gradient descent process.\n",
    "* **Role:** It determines how the parameters are updated at each iteration.\n",
    "* **Examples:** Stochastic Gradient Descent (SGD), Adam, RMSprop.\n",
    "\n",
    "### Best Fit Line: y = mx + c\n",
    "* **Definition:** The straight line that best represents the relationship between two variables on a scatter plot.\n",
    "* **Equation:** y = mx + c, where:\n",
    "    * y is the dependent variable\n",
    "    * x is the independent variable\n",
    "    * m is the slope of the line\n",
    "    * c is the y-intercept (the value of y when x is 0)\n",
    "\n",
    "### Slope (m)\n",
    "* **Definition:** The rate of change of the dependent variable with respect to the independent variable. It represents the steepness of the line.\n",
    "* **Interpretation:** A positive slope indicates a positive relationship between the variables, while a negative slope indicates a negative relationship.\n",
    "\n",
    "### Intercept (c)\n",
    "* **Definition:** The value of the dependent variable when the independent variable is zero. It's the point where the line crosses the y-axis.\n",
    "\n",
    "### Error (Residual)\n",
    "* **Definition:** The difference between the actual value of the dependent variable and the predicted value from the regression line.\n",
    "* **Role:** The goal of linear regression is to minimize the sum of squared errors.\n",
    "\n",
    "### Global Minima\n",
    "* **Definition:** The lowest point of a function over its entire domain.\n",
    "* **Goal:** In gradient descent, the aim is to find the global minimum of the error function to achieve the best model performance.\n",
    "\n",
    "### Mathematical Intuition of Linear Regression\n",
    "* **Objective:** To find the values of m and c that minimize the sum of squared errors between the observed data points and the predicted values on the line.\n",
    "* **Method:**\n",
    "    1. Initialize random values for m and c.\n",
    "    2. Calculate the error for each data point.\n",
    "    3. Calculate the gradient of the error function with respect to m and c.\n",
    "    4. Update m and c using gradient descent.\n",
    "    5. Repeat steps 2-4 until the error converges to a minimum.\n",
    "* **Underlying principle:** The line that minimizes the sum of squared errors is the best fit line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA (Exploratory Data Analysis)\n",
    "- **Definition**: EDA is the process of analyzing and summarizing the main characteristics of a dataset, often using visual methods. It helps in understanding the underlying structure, detecting anomalies or outliers, identifying patterns, and making decisions on how to handle data before applying machine learning models.\n",
    "- **Purpose**: EDA helps in making informed decisions about feature selection, transformation, and model selection, ultimately improving the performance of machine learning models.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "- **Definition**: MSE is a common loss function used in regression models to measure the average of the squares of the errors (i.e., the differences between the actual and predicted values). It gives a higher weight to larger errors.\n",
    "  * Formula: \n",
    "  ```\n",
    "  MSE = (1/n) * Σ(y_true - y_pred)^2\n",
    "  ```\n",
    "  where:\n",
    "  * n is the number of data points\n",
    "  * y_true is the true value\n",
    "  * y_pred is the predicted value\n",
    "- **Purpose**: MSE is used to evaluate the performance of regression models, with lower values indicating better model accuracy.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "- **Definition**: MAE is another loss function used in regression, which measures the average of the absolute differences between actual and predicted values. Unlike MSE, it treats all errors equally without giving extra weight to larger errors.\n",
    "  * Formula: \n",
    "  ```\n",
    "  MAE = (1/n) * Σ|y_true - y_pred|\n",
    "  ```\n",
    "\n",
    "- **Purpose**: MAE provides a straightforward measure of model accuracy, making it easier to interpret than MSE.\n",
    "\n",
    "### Outliers\n",
    "- **Definition**: Outliers are data points that significantly differ from the other observations in a dataset. They can result from measurement errors, data entry errors, or they might represent a real anomaly in the data.\n",
    "- **Purpose**: Identifying and understanding outliers is crucial as they can heavily influence the results of a model, potentially leading to misleading conclusions.\n",
    "\n",
    "### BoxPlot\n",
    "- **Definition**: A BoxPlot is a graphical representation used in EDA to display the distribution of a dataset based on five summary statistics: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It also identifies outliers by showing points outside the whiskers of the box.\n",
    "- **Purpose**: BoxPlots help in visualizing the spread, skewness, and potential outliers in the data, aiding in better data understanding during EDA.\n",
    "\n",
    "### How MAE is Robust to Outliers\n",
    "- **Explanation**: MAE is considered robust to outliers because it calculates the average absolute error without squaring the differences. This means that large errors (from outliers) do not disproportionately affect the overall error measure, unlike MSE, where outliers can have a significant impact due to the squaring of errors.\n",
    "- **Purpose**: Using MAE can provide a more balanced view of model performance, especially in datasets with outliers, making it a useful metric in scenarios where outliers are expected.\n",
    "\n",
    "### How They Help in EDA\n",
    "- **EDA**: By identifying outliers using tools like BoxPlots and understanding error metrics like MAE and MSE, you can make more informed decisions on how to preprocess your data. For example, you might decide to remove or transform outliers or choose a specific model evaluation metric based on the nature of your data. This process improves the quality of data fed into machine learning models, leading to better performance and more reliable results.\n",
    "\n",
    "These explanations should help you take clear and concise notes on these important machine learning concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "### What is Learning Rate?\n",
    "* A hyperparameter in machine learning algorithms, specifically optimization algorithms like gradient descent.\n",
    "* Determines the step size taken in the direction of the negative gradient during each iteration.\n",
    "* Controls how quickly the model's parameters are updated.\n",
    "\n",
    "### Where is it Used?\n",
    "* Primarily used in gradient-based optimization algorithms, including:\n",
    "    * Gradient Descent\n",
    "    * Stochastic Gradient Descent (SGD)\n",
    "    * Adam\n",
    "    * RMSprop\n",
    "\n",
    "### Impact of Learning Rate\n",
    "\n",
    "* **Too Low Learning Rate:**\n",
    "    * Slow convergence: Model takes a long time to reach the optimal solution.\n",
    "    * Risk of getting stuck in local minima.\n",
    "    * Inefficient training process.\n",
    "\n",
    "* **Just Right Learning Rate:**\n",
    "    * Optimal convergence: Model reaches the optimal solution efficiently.\n",
    "    * Good balance between speed and accuracy.\n",
    "\n",
    "* **Too High Learning Rate:**\n",
    "    * Divergence: Model's parameters oscillate and fail to converge.\n",
    "    * Overfitting: Model becomes too sensitive to training data.\n",
    "    * Instability in the training process.\n",
    "\n",
    "**Note:** Finding the optimal learning rate is crucial for model performance. Techniques like learning rate scheduling and adaptive learning rate methods can help in fine-tuning the learning rate during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions in Linear Regression\n",
    "\n",
    "Linear regression is a powerful statistical tool, but it relies on certain assumptions about the data. These assumptions are crucial for the validity of the model and its inferences. Let's explore them:\n",
    "\n",
    "### 1. Linearity\n",
    "* **Assumption:** There is a linear relationship between the dependent variable and the independent variables.\n",
    "* **Implications:** If the relationship is not linear, the model will be inaccurate.\n",
    "* **Checking:** Scatter plots can help visualize the relationship.\n",
    "\n",
    "### 2. Independence\n",
    "* **Assumption:** The observations are independent of each other.\n",
    "* **Implications:** If observations are correlated, the model's estimates might be biased.\n",
    "* **Checking:** Time series data often violates this assumption. It can be checked using autocorrelation plots.\n",
    "\n",
    "### 3. Homoscedasticity\n",
    "* **Assumption:** The variance of the error terms is constant across all levels of the independent variable.\n",
    "* **Implications:** If the variance is not constant (heteroscedasticity), the model's estimates might be inefficient.\n",
    "* **Checking:** Residual plots can help identify non-constant variance.\n",
    "\n",
    "### 4. Normality\n",
    "* **Assumption:** The error terms follow a normal distribution.\n",
    "* **Implications:** This assumption is important for inference and hypothesis testing.\n",
    "* **Checking:** Histograms and Q-Q plots of residuals can assess normality.\n",
    "\n",
    "### 5. No Multicollinearity\n",
    "* **Assumption:** There is no perfect linear relationship between the independent variables.\n",
    "* **Implications:** Multicollinearity can inflate standard errors and make it difficult to interpret the model.\n",
    "* **Checking:** Correlation matrix, Variance Inflation Factor (VIF), and Tolerance can be used.\n",
    "\n",
    "### 6. No Autocorrelation\n",
    "* **Assumption:** The error terms are uncorrelated with each other.\n",
    "* **Implications:** Autocorrelation often occurs in time series data and can lead to inefficient estimates.\n",
    "* **Checking:** Durbin-Watson test can be used.\n",
    "\n",
    "**Note:** While these assumptions are important, it's essential to remember that no real-world data perfectly meets all assumptions. The key is to identify potential violations and address them appropriately, either through data transformations, using robust methods, or considering alternative models.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
