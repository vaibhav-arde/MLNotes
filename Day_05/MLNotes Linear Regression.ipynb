{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression related terms\n",
    "\n",
    "**Note:** Linear regression is a foundational algorithm in machine learning, providing a solid understanding of the core concepts. It's essential to grasp these terms to delve deeper into more complex models and techniques.\n",
    "\n",
    "### Linear Regression\n",
    "* **Definition:** A statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (predictors) by fitting a linear equation to the observed data.\n",
    "* **Goal:** To find the best-fitting line that minimizes the difference between the predicted values and the actual values.\n",
    "\n",
    "### Gradient Descent\n",
    "* **Definition:** An optimization algorithm used to find the minimum of a function. In machine learning, it's used to find the optimal parameters (weights and biases) of a model.\n",
    "* **Process:** It iteratively adjusts the parameters in the direction of steepest descent (negative gradient) of the error function until a minimum is reached.\n",
    "\n",
    "### Gradient Descent Optimizer\n",
    "* **Definition:** An algorithm that implements the gradient descent process.\n",
    "* **Role:** It determines how the parameters are updated at each iteration.\n",
    "* **Examples:** Stochastic Gradient Descent (SGD), Adam, RMSprop.\n",
    "\n",
    "### Best Fit Line: y = mx + c\n",
    "* **Definition:** The straight line that best represents the relationship between two variables on a scatter plot.\n",
    "* **Equation:** y = mx + c, where:\n",
    "    * y is the dependent variable\n",
    "    * x is the independent variable\n",
    "    * m is the slope of the line\n",
    "    * c is the y-intercept (the value of y when x is 0)\n",
    "\n",
    "### Slope (m)\n",
    "* **Definition:** The rate of change of the dependent variable with respect to the independent variable. It represents the steepness of the line.\n",
    "* **Interpretation:** A positive slope indicates a positive relationship between the variables, while a negative slope indicates a negative relationship.\n",
    "\n",
    "### Intercept (c)\n",
    "* **Definition:** The value of the dependent variable when the independent variable is zero. It's the point where the line crosses the y-axis.\n",
    "\n",
    "### Error (Residual)\n",
    "* **Definition:** The difference between the actual value of the dependent variable and the predicted value from the regression line.\n",
    "* **Role:** The goal of linear regression is to minimize the sum of squared errors.\n",
    "\n",
    "### Global Minima\n",
    "* **Definition:** The lowest point of a function over its entire domain.\n",
    "* **Goal:** In gradient descent, the aim is to find the global minimum of the error function to achieve the best model performance.\n",
    "\n",
    "### Mathematical Intuition of Linear Regression\n",
    "* **Objective:** To find the values of m and c that minimize the sum of squared errors between the observed data points and the predicted values on the line.\n",
    "* **Method:**\n",
    "    1. Initialize random values for m and c.\n",
    "    2. Calculate the error for each data point.\n",
    "    3. Calculate the gradient of the error function with respect to m and c.\n",
    "    4. Update m and c using gradient descent.\n",
    "    5. Repeat steps 2-4 until the error converges to a minimum.\n",
    "* **Underlying principle:** The line that minimizes the sum of squared errors is the best fit line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA (Exploratory Data Analysis)\n",
    "- **Definition**: EDA is the process of analyzing and summarizing the main characteristics of a dataset, often using visual methods. It helps in understanding the underlying structure, detecting anomalies or outliers, identifying patterns, and making decisions on how to handle data before applying machine learning models.\n",
    "- **Purpose**: EDA helps in making informed decisions about feature selection, transformation, and model selection, ultimately improving the performance of machine learning models.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "- **Definition**: MSE is a common loss function used in regression models to measure the average of the squares of the errors (i.e., the differences between the actual and predicted values). It gives a higher weight to larger errors.\n",
    "  * Formula: \n",
    "  ```\n",
    "  MSE = (1/n) * Σ(y_true - y_pred)^2\n",
    "  ```\n",
    "  where:\n",
    "  * n is the number of data points\n",
    "  * y_true is the true value\n",
    "  * y_pred is the predicted value\n",
    "- **Purpose**: MSE is used to evaluate the performance of regression models, with lower values indicating better model accuracy.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "- **Definition**: MAE is another loss function used in regression, which measures the average of the absolute differences between actual and predicted values. Unlike MSE, it treats all errors equally without giving extra weight to larger errors.\n",
    "  * Formula: \n",
    "  ```\n",
    "  MAE = (1/n) * Σ|y_true - y_pred|\n",
    "  ```\n",
    "\n",
    "- **Purpose**: MAE provides a straightforward measure of model accuracy, making it easier to interpret than MSE.\n",
    "\n",
    "### Outliers\n",
    "- **Definition**: Outliers are data points that significantly differ from the other observations in a dataset. They can result from measurement errors, data entry errors, or they might represent a real anomaly in the data.\n",
    "- **Purpose**: Identifying and understanding outliers is crucial as they can heavily influence the results of a model, potentially leading to misleading conclusions.\n",
    "\n",
    "### BoxPlot\n",
    "- **Definition**: A BoxPlot is a graphical representation used in EDA to display the distribution of a dataset based on five summary statistics: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It also identifies outliers by showing points outside the whiskers of the box.\n",
    "- **Purpose**: BoxPlots help in visualizing the spread, skewness, and potential outliers in the data, aiding in better data understanding during EDA.\n",
    "\n",
    "### How MAE is Robust to Outliers\n",
    "- **Explanation**: MAE is considered robust to outliers because it calculates the average absolute error without squaring the differences. This means that large errors (from outliers) do not disproportionately affect the overall error measure, unlike MSE, where outliers can have a significant impact due to the squaring of errors.\n",
    "- **Purpose**: Using MAE can provide a more balanced view of model performance, especially in datasets with outliers, making it a useful metric in scenarios where outliers are expected.\n",
    "\n",
    "### How They Help in EDA\n",
    "- **EDA**: By identifying outliers using tools like BoxPlots and understanding error metrics like MAE and MSE, you can make more informed decisions on how to preprocess your data. For example, you might decide to remove or transform outliers or choose a specific model evaluation metric based on the nature of your data. This process improves the quality of data fed into machine learning models, leading to better performance and more reliable results.\n",
    "\n",
    "These explanations should help you take clear and concise notes on these important machine learning concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "### What is Learning Rate?\n",
    "* A hyperparameter in machine learning algorithms, specifically optimization algorithms like gradient descent.\n",
    "* Determines the step size taken in the direction of the negative gradient during each iteration.\n",
    "* Controls how quickly the model's parameters are updated.\n",
    "\n",
    "### Where is it Used?\n",
    "* Primarily used in gradient-based optimization algorithms, including:\n",
    "    * Gradient Descent\n",
    "    * Stochastic Gradient Descent (SGD)\n",
    "    * Adam\n",
    "    * RMSprop\n",
    "\n",
    "### Impact of Learning Rate\n",
    "\n",
    "* **Too Low Learning Rate:**\n",
    "    * Slow convergence: Model takes a long time to reach the optimal solution.\n",
    "    * Risk of getting stuck in local minima.\n",
    "    * Inefficient training process.\n",
    "\n",
    "* **Just Right Learning Rate:**\n",
    "    * Optimal convergence: Model reaches the optimal solution efficiently.\n",
    "    * Good balance between speed and accuracy.\n",
    "\n",
    "* **Too High Learning Rate:**\n",
    "    * Divergence: Model's parameters oscillate and fail to converge.\n",
    "    * Overfitting: Model becomes too sensitive to training data.\n",
    "    * Instability in the training process.\n",
    "\n",
    "**Note:** Finding the optimal learning rate is crucial for model performance. Techniques like learning rate scheduling and adaptive learning rate methods can help in fine-tuning the learning rate during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions in Linear Regression\n",
    "\n",
    "Linear regression is a powerful statistical tool, but it relies on certain assumptions about the data. These assumptions are crucial for the validity of the model and its inferences. Let's explore them:\n",
    "\n",
    "### 1. Linearity\n",
    "* **Assumption:** There is a linear relationship between the dependent variable and the independent variables.\n",
    "* **Implications:** If the relationship is not linear, the model will be inaccurate.\n",
    "* **Checking:** Scatter plots can help visualize the relationship.\n",
    "\n",
    "### 2. Independence\n",
    "* **Assumption:** The observations are independent of each other.\n",
    "* **Implications:** If observations are correlated, the model's estimates might be biased.\n",
    "* **Checking:** Time series data often violates this assumption. It can be checked using autocorrelation plots.\n",
    "\n",
    "### 3. Homoscedasticity\n",
    "* **Assumption:** The variance of the error terms is constant across all levels of the independent variable.\n",
    "* **Implications:** If the variance is not constant (heteroscedasticity), the model's estimates might be inefficient.\n",
    "* **Checking:** Residual plots can help identify non-constant variance.\n",
    "\n",
    "### 4. Normality\n",
    "* **Assumption:** The error terms follow a normal distribution.\n",
    "* **Implications:** This assumption is important for inference and hypothesis testing.\n",
    "* **Checking:** Histograms and Q-Q plots of residuals can assess normality.\n",
    "\n",
    "### 5. No Multicollinearity\n",
    "* **Assumption:** There is no perfect linear relationship between the independent variables.\n",
    "* **Implications:** Multicollinearity can inflate standard errors and make it difficult to interpret the model.\n",
    "* **Checking:** Correlation matrix, Variance Inflation Factor (VIF), and Tolerance can be used.\n",
    "\n",
    "### 6. No Autocorrelation\n",
    "* **Assumption:** The error terms are uncorrelated with each other.\n",
    "* **Implications:** Autocorrelation often occurs in time series data and can lead to inefficient estimates.\n",
    "* **Checking:** Durbin-Watson test can be used.\n",
    "\n",
    "**Note:** While these assumptions are important, it's essential to remember that no real-world data perfectly meets all assumptions. The key is to identify potential violations and address them appropriately, either through data transformations, using robust methods, or considering alternative models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "**Definition**: Linear Regression is a statistical method used in machine learning to model the relationship between a dependent variable (target) and one or more independent variables (features). The goal is to find the best-fitting linear equation that predicts the target variable from the features.\n",
    "\n",
    "**Mathematical Equation**:\n",
    "- For a single feature (Simple Linear Regression): \\( y = mx + c \\)\n",
    "- For multiple features (Multiple Linear Regression): \\( y = b_0 + b_1x_1 + b_2x_2 + \\dots + b_nx_n \\)\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the predicted value.\n",
    "- \\( x \\) is the input feature.\n",
    "- \\( m \\) or \\( b_1, b_2, \\dots, b_n \\) are the coefficients (slopes).\n",
    "- \\( c \\) or \\( b_0 \\) is the intercept (constant term).\n",
    "\n",
    "### Types of Linear Regression\n",
    "\n",
    "### 1. **Simple Linear Regression**\n",
    "   - **Definition**: Simple linear regression models the relationship between a single independent variable (feature) and a dependent variable (target) using a straight line.\n",
    "   - **Equation**: \\( y = mx + c \\)\n",
    "   - **Example**:\n",
    "     - Predicting house prices based on the size of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: 263525.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1400], [1600], [1700], [1875], [1100]])  # Size of the house\n",
    "y = np.array([245000, 312000, 279000, 308000, 199000])  # Price of the house\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the price of a new house\n",
    "new_house_size = np.array([[1500]])\n",
    "price_prediction = model.predict(new_house_size)\n",
    "print(f\"Predicted price: {price_prediction[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Multiple Linear Regression**\n",
    "   - **Definition**: Multiple linear regression models the relationship between two or more independent variables (features) and a dependent variable (target). The goal is to find the best-fitting linear equation that describes the relationship.\n",
    "   - **Equation**: y = b0 + b1x1 + b2x2 + ... + bnxn\n",
    "   - **Example**:\n",
    "     - Predicting house prices based on multiple factors like size, number of bedrooms, and location.\n",
    "     - **Code Example using scikit-learn**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: 259098.43\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "# Sample data\n",
    "X = np.array([[1400, 3], [1600, 4], [1700, 3], [1875, 4], [1100, 2]])  # Size of the house and number of bedrooms\n",
    "y = np.array([245000, 312000, 279000, 308000, 199000])  # Price of the house\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "# Predict the price of a new house\n",
    "new_house = np.array([[1500, 3]])\n",
    "price_prediction = model.predict(new_house)\n",
    "print(f\"Predicted price: {price_prediction[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Polynomial Regression**\n",
    "   - **Definition**: Polynomial regression is an extension of linear regression where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial. It can capture non-linear relationships between the features and the target.\n",
    "   - **Equation**: y = b0 + b1x1 + b2x1^2 + ... + bnx1^n\n",
    "   - **Example**:\n",
    "     - Predicting the trajectory of a projectile where the relationship between time and distance is quadratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted distance: 36.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3], [4], [5]])  # Time\n",
    "y = np.array([1, 4, 9, 16, 25])  \n",
    "# Distance (quadratic relationship)\n",
    "# Transform the data to include polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "# Predict the distance at a new time\n",
    "new_time = np.array([[6]])\n",
    "new_time_poly = poly.transform(new_time)\n",
    "distance_prediction = model.predict(new_time_poly)\n",
    "print(f\"Predicted distance: {distance_prediction[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary for Notes\n",
    "- **Linear Regression**: A method to model the relationship between a dependent variable and one or more independent variables using a linear equation.\n",
    "  - **Simple Linear Regression**: Models a single feature's impact on the target (e.g., house price prediction based on size).\n",
    "  - **Multiple Linear Regression**: Models multiple features' impacts on the target (e.g., house price prediction based on size, bedrooms, and location).\n",
    "  - **Polynomial Regression**: Captures non-linear relationships by modeling the data with an nth degree polynomial (e.g., predicting projectile trajectory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Squared (R²)\n",
    "\n",
    "**Definition**: R-Squared, also known as the coefficient of determination, is a statistical measure that indicates the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features). It represents how well the regression model fits the observed data.\n",
    "\n",
    "**Mathematical Formula**:\n",
    "R^2 = 1 - (SSres / SStot)\n",
    "\n",
    "Where:\n",
    "- SSres (Residual Sum of Squares): Sum of the squared differences between the observed values and the predicted values.\n",
    "- SStot (Total Sum of Squares): Sum of the squared differences between the observed values and the mean of the observed values.\n",
    "\n",
    "**Interpretation**:\n",
    "- \\( R^2 = 1 \\): The model perfectly predicts the target variable, meaning all the variance in the target is explained by the features.\n",
    "- \\( R^2 = 0 \\): The model does not explain any of the variance in the target variable, meaning the model’s predictions are as good as the mean of the target variable.\n",
    "- **Value Range**: \\( R^2 \\) ranges from 0 to 1, where higher values indicate a better fit.\n",
    "\n",
    "**Use in Performance Metric**:\n",
    "- R-Squared helps determine how well the independent variables explain the variability of the dependent variable. It provides a quick assessment of model performance, especially in linear regression models.\n",
    "\n",
    "## Adjusted R-Squared\n",
    "\n",
    "**Definition**: Adjusted R-Squared is a modified version of R-Squared that adjusts for the number of independent variables in the model. Unlike R-Squared, which can increase as more variables are added to the model (even if they are not significant), Adjusted R-Squared accounts for the model's complexity and only increases if the added variables improve the model.\n",
    "\n",
    "**Mathematical Formula**:\n",
    "Adjusted R^2 = 1 - ((1-R^2)(n-1)/(n - p - 1))\n",
    "\n",
    "\n",
    "Where:\n",
    "- \\( n \\) is the number of observations.\n",
    "- \\( p \\) is the number of predictors (independent variables).\n",
    "- \\( R^2 \\) is the R-Squared value.\n",
    "\n",
    "**Interpretation**:\n",
    "- **Penalizes Complexity**: Adjusted R-Squared decreases if the added variables do not improve the model, helping to avoid overfitting.\n",
    "- **Better Comparison**: It allows for a more accurate comparison between models with a different number of predictors, as it accounts for the model's complexity.\n",
    "\n",
    "**Use in Performance Metric**:\n",
    "- Adjusted R-Squared is particularly useful in multiple regression models where the number of predictors may vary. It helps in selecting the right model by balancing model fit and complexity.\n",
    "\n",
    "### Summary for Notes\n",
    "- **R-Squared (R²)**: Measures the proportion of variance in the target variable explained by the independent variables. `Formula: R^2 = 1 - (SSres / SStot) `A higher \\( R^2 \\) indicates a better model fit.\n",
    "- **Adjusted R-Squared**: Adjusts the R-Squared value based on the number of predictors, penalizing the addition of non-significant variables. `Formula: Adjusted R^2 = 1 - ((1-R^2)(n-1)/(n - p - 1))` It provides a more accurate performance metric, especially for multiple regression models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
