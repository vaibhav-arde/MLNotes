{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Q1: Why should you not use accuracy for model evaluation in an imbalanced dataset?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Accuracy is not a good metric for evaluating models on imbalanced datasets. In a dataset where 90% of the data belongs to class 0 and 10% belongs to class 1, a model that predicts everything as class 0 would still have 90% accuracy, despite completely failing to identify the minority class (class 1). This leads to **misleading results** because accuracy doesn’t consider the distribution of classes.\n",
    "\n",
    "**Accuracy Formula**:\n",
    "\n",
    "```python\n",
    "Accuracy = (True Positives + True Negatives) / (Total Samples)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **True Positives (TP)**: Correct predictions for the positive class.\n",
    "- **True Negatives (TN)**: Correct predictions for the negative class.\n",
    "\n",
    "In imbalanced datasets, this formula often gives a high value, even if the model fails to correctly identify the minority class.\n",
    "\n",
    "Instead, other metrics like **Precision, Recall, F1-Score, and AUC-ROC** are more informative in evaluating performance, particularly for the minority class.\n",
    "\n",
    "- **Precision**: Measures how many of the predicted positive cases are actually positive.\n",
    "- **Recall**: Measures how many of the actual positive cases were predicted correctly.\n",
    "- **F1-Score**: Harmonic mean of Precision and Recall, balancing both metrics.\n",
    "- **AUC-ROC**: Evaluates how well the model can distinguish between the two classes by plotting True Positive Rate (Recall) against False Positive Rate.\n",
    "\n",
    "**Example response for an interview**:  \n",
    "_\"In an imbalanced classification problem, accuracy can give an illusion of high performance when the model is just predicting the majority class. Instead, metrics like Precision, Recall, and F1-Score are more effective in evaluating performance, especially in identifying the minority class. AUC-ROC can also help by showing how well the model discriminates between classes.\"_\n",
    "\n",
    "**Python Implementation Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Example true and predicted values\n",
    "y_true = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "```\n",
    "\n",
    "In the above example, despite the **accuracy** being decent, metrics like **precision**, **recall**, and **F1-score** will provide a better picture for imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2: Explain oversampling and how it can help with imbalanced data.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Oversampling** is a technique used to handle imbalanced datasets by increasing the number of instances in the minority class. This is done by **duplicating** samples or creating synthetic samples. Oversampling ensures that the model is trained on a more balanced dataset, preventing it from being biased towards the majority class.\n",
    "\n",
    "**Popular Oversampling Techniques:**\n",
    "1. **Random Oversampling**: Randomly duplicates samples from the minority class until the classes are balanced. While simple, it can lead to **overfitting** because the model may memorize these duplicated instances.\n",
    "2. **SMOTE (Synthetic Minority Over-sampling Technique)**: A more advanced technique where new synthetic samples are created by interpolating between existing minority class samples.\n",
    "\n",
    "**Example response for an interview**:  \n",
    "_\"Oversampling helps in balancing the data by either duplicating or synthesizing more minority class samples, allowing the model to pay more attention to the minority class. Techniques like SMOTE are widely used to generate synthetic data points for the minority class, thereby improving the model's ability to generalize.\"_\n",
    "\n",
    "**Formula** (for SMOTE synthetic samples creation):\n",
    "\n",
    "```python\n",
    "New Sample = Sample_min + λ * (Sample_nearest - Sample_min)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Sample_min`: A sample from the minority class.\n",
    "- `Sample_nearest`: The nearest neighbor of `Sample_min`.\n",
    "- `λ`: A random number between 0 and 1.\n",
    "\n",
    "**Python Implementation (using SMOTE):**\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1,\n",
    "                           weights=[0.9], flip_y=0, random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "# Plot original vs resampled data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=y)\n",
    "axs[0].set_title('Original Data')\n",
    "\n",
    "axs[1].scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)\n",
    "axs[1].set_title('Resampled Data (SMOTE)')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3: Explain undersampling and its benefits and drawbacks.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Undersampling** is the opposite of oversampling. It reduces the size of the majority class to balance the dataset. By removing some of the majority class instances, we can make the dataset more balanced and avoid biasing the model towards the majority class.\n",
    "\n",
    "**Benefits:**\n",
    "- It is computationally less expensive since the dataset becomes smaller.\n",
    "- Can work well when there’s a lot of redundancy in the majority class.\n",
    "\n",
    "**Drawbacks:**\n",
    "- By removing data, you risk losing important information from the majority class.\n",
    "- This can lead to **underfitting**, where the model doesn't learn enough because important patterns in the majority class may be discarded.\n",
    "\n",
    "**Example response for an interview**:  \n",
    "_\"Undersampling is useful when we have large datasets, as it reduces the training data size by removing instances from the majority class. While it balances the dataset, it comes with the risk of losing valuable data from the majority class, potentially underfitting the model.\"_\n",
    "\n",
    "**Example Formula** for undersampling:  \n",
    "If we have 1000 samples in the majority class and 100 in the minority class, we randomly select 100 samples from the majority class to create a balanced dataset.\n",
    "\n",
    "**Python Implementation Example:**\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Original dataset (same as above)\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1,\n",
    "                           weights=[0.9], flip_y=0, random_state=42)\n",
    "\n",
    "# Apply Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# Plot original vs resampled data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=y)\n",
    "axs[0].set_title('Original Data')\n",
    "\n",
    "axs[1].scatter(X_resampled[:, 0], X_resampled[:, 1], c=y_resampled)\n",
    "axs[1].set_title('Resampled Data (Undersampling)')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4: What is SMOTE, and how does it work?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)** is an oversampling technique that creates synthetic examples rather than simply duplicating minority class samples. \n",
    "\n",
    "**How it works:**\n",
    "- For each minority class sample, SMOTE selects its **k-nearest neighbors**.\n",
    "- It then creates a new synthetic sample by randomly choosing one of the neighbors and generating a point along the line connecting the two samples.\n",
    "- This way, it generates new instances that are not mere copies but fall in between existing minority class examples.\n",
    "\n",
    "**Benefits:**\n",
    "- **Reduces overfitting**: Since it generates new synthetic samples rather than duplicating data, the model doesn’t memorize the data.\n",
    "- **Balances the data**: Improves model performance by making the data distribution more balanced.\n",
    "\n",
    "**Example response for an interview**:  \n",
    "_\"SMOTE is a technique that generates synthetic samples for the minority class by interpolating between existing samples and their nearest neighbors. This helps the model to generalize better and avoid overfitting, which can occur in simple random oversampling methods.\"_ \n",
    "\n",
    "**Formula** (same as in Q2):\n",
    "\n",
    "```python\n",
    "New Sample = Sample_min + λ * (Sample_nearest - Sample_min)\n",
    "```\n",
    "\n",
    "**Python Implementation**: (already provided in Q2)\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5: How can you alter the cost function to address imbalanced data?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "In **imbalanced classification problems**, we can **alter the cost function** of the model to penalize misclassifying the minority class more than the majority class. This approach assigns **higher weights** to the minority class during training, which forces the model to focus more on correctly predicting minority class samples.\n",
    "\n",
    "**Examples of altering cost functions:**\n",
    "1. **Weighted Loss Function**: In algorithms like Logistic Regression or SVM, we can add class weights to the loss function. This means that the model will incur a larger penalty for misclassifying minority class samples, forcing it to pay more attention to those samples.\n",
    "    - In Scikit-Learn, you can use `class_weight='balanced'` in many classifiers (like Logistic Regression, SVM, etc.) to automatically adjust the weights inversely proportional to class frequencies.\n",
    "2. **Focal Loss** (used in deep learning): A variant of cross-entropy loss that adds a modulating term to focus learning more on hard-to-classify examples (usually from the minority class).\n",
    "\n",
    "**Mathematical Intuition** (for weighted loss):\n",
    "Let `W_0` and `W_1` represent the weights assigned to class 0 and class 1, respectively. The loss function can be modified as:\n",
    "\n",
    "```python\n",
    "L = W_0 * L_0 + W_1 * L_1\n",
    "```\n",
    "Where:\n",
    "- `L_0` and `L_1` represent the individual loss for class 0 and class 1.\n",
    "- `W_0` and `W_1` are inversely proportional to the class frequencies.\n",
    "\n",
    "**Example response for an interview**:  \n",
    "_\"By altering the cost function, we can assign higher weights to the minority class, ensuring the model pays more attention to misclassifications in that class. This technique works well in logistic regression, SVMs, and even deep learning models, using weighted loss or focal loss.\"_ \n",
    "\n",
    "**Python Implementation Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
    "                           n_redundant=10, weights=[0.9], flip_y=0, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply weighted classifier\n",
    "clf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "In this implementation, the `class_weight='balanced'` parameter automatically assigns weights to classes inversely proportional to their frequencies in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Formulas and Python Examples**\n",
    "\n",
    "1. **Accuracy Formula**:\n",
    "   **Why accuracy is a poor metric**: It can mislead you into thinking your model is performing well even if it's just predicting the majority class.\n",
    "   - Formula: `Accuracy = (TP + TN) / (Total Samples)`\n",
    "\n",
    "   Python: `accuracy_score`\n",
    "\n",
    "2. **Oversampling Formula (SMOTE)**: Duplicates or generates synthetic samples for the minority class.\n",
    "   ```python\n",
    "   New Sample = Sample_min + λ * (Sample_nearest - Sample_min)\n",
    "   ```\n",
    "   Python: `SMOTE`\n",
    "\n",
    "3. **Undersampling Formula**: Reduces the number of majority class instances to balance the dataset.\n",
    "\n",
    "   No specific formula; it involves randomly removing instances from the majority class.\n",
    "\n",
    "4. **Weighted Loss Formula**: Generates synthetic data points for the minority class to balance the dataset and reduce overfitting.\n",
    "\n",
    "   ```python\n",
    "   Weighted Loss = W_0 * Loss_0 + W_1 * Loss_1\n",
    "   ```\n",
    "   Python: `class_weight='balanced'`\n",
    "\n",
    "5. **Altering cost function**: Assign higher weights to the minority class during training to make the model focus on correctly predicting minority class examples.\n",
    "\n",
    "Each technique has its advantages and drawbacks, and in practice, combining these methods (like oversampling with altering the cost function) can often yield the best results.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
