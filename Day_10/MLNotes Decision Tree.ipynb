{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decision Tree: Overview**\n",
    "\n",
    "A **Decision Tree** is a supervised learning algorithm used for both classification and regression tasks. It works by recursively splitting the dataset into subsets based on the value of input features. The splits are chosen in such a way that they create the most homogeneous groups with respect to the target variable.\n",
    "\n",
    "A decision tree resembles a tree structure with nodes representing different decision points, and the branches representing outcomes. The tree is constructed from root to leaves, where:\n",
    "- **Root Node**: Represents the entire dataset and the best split.\n",
    "- **Internal/Decision Nodes**: Represent a test on an attribute and possible outcomes (branches).\n",
    "- **Leaf/Terminal Nodes**: Represent the final prediction or decision (class label or value).\n",
    "\n",
    "---\n",
    "\n",
    "### **Components of a Decision Tree**\n",
    "\n",
    "#### **1. Root Node**\n",
    "- The top node of a decision tree.\n",
    "- Represents the entire dataset before any splits.\n",
    "- The split at this point results in the maximum reduction in impurity (e.g., Gini, entropy).\n",
    "\n",
    "#### **2. Decision Nodes**\n",
    "- Internal nodes that represent the feature upon which the dataset is split.\n",
    "- Each internal node splits the dataset based on a feature, and the decision made determines which branch to follow.\n",
    "\n",
    "#### **3. Leaf Nodes (Terminal Nodes)**\n",
    "- Nodes that do not split further.\n",
    "- Each leaf represents a class label (in classification) or a value (in regression).\n",
    "- All the data points reaching a particular leaf node belong to the same class (or approximate value in regression).\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Nodes**\n",
    "\n",
    "1. **Root Node**: The top-most node in a decision tree, representing the entire dataset before any split.\n",
    "2. **Internal/Decision Node**: A node representing a decision point where the data is split based on a feature.\n",
    "3. **Leaf Node**: The end node that provides a classification or regression outcome.\n",
    "\n",
    "---\n",
    "\n",
    "### **Impurity in Decision Trees**\n",
    "\n",
    "Impurity refers to the degree of disorder or randomness in a dataset. When splitting nodes, decision trees aim to reduce impurity to create homogeneous branches (subsets). Different metrics can be used to measure impurity:\n",
    "\n",
    "#### **1. Gini Impurity**\n",
    "\n",
    "The **Gini Impurity** measures how often a randomly chosen element from the set would be incorrectly classified if it were randomly classified according to the distribution of class labels in the set.\n",
    "\n",
    "- **Formula**:\n",
    "```python\n",
    "Gini = 1 - Σ(p_i^2)\n",
    "```\n",
    "Where `p_i` is the probability of class `i`.\n",
    "\n",
    "- **Range**: The value of Gini impurity ranges from 0 (pure node, all elements are of the same class) to 0.5 (impure node, equal distribution of classes).\n",
    "\n",
    "#### **2. Entropy (Information Gain)**\n",
    "\n",
    "**Entropy** is a measure from information theory that quantifies the amount of uncertainty or impurity in the data. The goal is to reduce entropy as the tree grows.\n",
    "\n",
    "- **Formula**:\n",
    "```python\n",
    "Entropy = - Σ(p_i * log2(p_i))\n",
    "```\n",
    "Where `p_i` is the probability of class `i`.\n",
    "\n",
    "- **Range**: Entropy values range from 0 (pure node) to 1 (maximum impurity for binary classification).\n",
    "\n",
    "- **Information Gain**: It represents the reduction in entropy after the dataset is split on an attribute.\n",
    "  - **Formula**:\n",
    "```python\n",
    "Information Gain = Entropy(parent) - Weighted Sum of Entropy(children)\n",
    "```\n",
    "\n",
    "#### **3. Mean Squared Error (MSE)**\n",
    "\n",
    "For **regression trees**, the impurity measure is typically **Mean Squared Error (MSE)**. The goal is to minimize the variance within each node, which is equivalent to reducing the MSE.\n",
    "\n",
    "- **Formula**:\n",
    "```python\n",
    "MSE = (1/N) * Σ(y_i - ŷ)^2\n",
    "```\n",
    "Where `y_i` is the actual value, and `ŷ` is the predicted value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Splitting Criteria**\n",
    "\n",
    "When constructing a decision tree, the algorithm evaluates different features and thresholds to determine the best split. This is done by calculating the **impurity** (Gini, entropy, or MSE) for each possible split and selecting the one that reduces impurity the most. The process continues recursively until a stopping criterion is met (e.g., maximum depth of the tree, minimum number of samples in a node, etc.).\n",
    "\n",
    "1. **Gini Impurity** is typically used in **CART (Classification and Regression Trees)**.\n",
    "2. **Entropy/Information Gain** is often used in **ID3 and C4.5** decision tree algorithms.\n",
    "3. **MSE** is used for **regression trees** to measure the variance in the data at each split.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of a Decision Tree**\n",
    "\n",
    "Consider the task of predicting whether a person will buy a car based on income and age:\n",
    "\n",
    "1. **Root Node**: The dataset is split based on the feature `income`. People with income > `X` go one way, others go another way.\n",
    "2. **Decision Nodes**: The `age` feature is used to further split the data into younger and older individuals.\n",
    "3. **Leaf Nodes**: After splitting based on income and age, the final leaf nodes represent predictions such as \"Buys car\" or \"Does not buy car.\"\n",
    "\n",
    "Each split tries to make the nodes purer by reducing the impurity (Gini, Entropy, or MSE).\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "- **Decision Trees** are interpretable, easy to visualize, and versatile in handling both classification and regression problems.\n",
    "- **Impurity** measures like **Gini** and **Entropy** guide the splitting process to create homogeneous nodes.\n",
    "- **Leaf Nodes** provide the final predictions, and the **decision nodes** make logical choices that direct the flow of the tree.\n",
    "\n",
    "The tree structure makes it easy to understand how decisions are made and how the model arrives at its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gini Index**\n",
    "\n",
    "The **Gini Index** (or **Gini Impurity**) is a metric used in decision trees to measure the impurity of a node. It represents the probability that a randomly chosen element from the dataset would be incorrectly classified if it were randomly labeled according to the class distribution at that node.\n",
    "\n",
    "The goal of a decision tree algorithm is to split the nodes in such a way that the resulting nodes have lower impurity. The **Gini Index** helps quantify the impurity before and after a split, allowing the algorithm to choose the best possible split.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula for Gini Index**\n",
    "\n",
    "For a node with `k` possible classes (categories), the Gini Index is defined as:\n",
    "\n",
    "``` \n",
    "Gini = 1 - Σ(p_i^2)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `p_i` is the proportion of instances that belong to class `i` at a particular node.\n",
    "\n",
    "### **Interpretation of Gini Index**\n",
    "\n",
    "- **Gini = 0**: This means the node is **pure**; all elements belong to a single class.\n",
    "- **Gini > 0**: The node contains elements from multiple classes, indicating impurity.\n",
    "- **Maximum Gini (e.g., Gini = 0.5 for binary classification)**: This occurs when classes are evenly distributed, meaning the node is highly impure.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Calculation of Gini Index**\n",
    "\n",
    "Imagine a node where the dataset has two classes (binary classification), `Class 0` and `Class 1`.\n",
    "\n",
    "#### **Example 1: Pure Node**\n",
    "Suppose the node contains 10 instances, all of which belong to `Class 0`. The proportions are:\n",
    "- `p_0 = 1.0` (100% belong to Class 0)\n",
    "- `p_1 = 0.0` (0% belong to Class 1)\n",
    "\n",
    "The Gini Index would be:\n",
    "\n",
    "``` \n",
    "Gini = 1 - (1.0^2 + 0.0^2) \n",
    "     = 1 - 1.0 \n",
    "     = 0\n",
    "```\n",
    "This indicates a **pure node** (all elements belong to one class).\n",
    "\n",
    "#### **Example 2: Impure Node**\n",
    "Now, suppose the node contains 10 instances, with 4 instances of `Class 0` and 6 instances of `Class 1`. The proportions are:\n",
    "- `p_0 = 4/10 = 0.4`\n",
    "- `p_1 = 6/10 = 0.6`\n",
    "\n",
    "The Gini Index would be:\n",
    "\n",
    "``` \n",
    "Gini = 1 - (0.4^2 + 0.6^2) \n",
    "     = 1 - (0.16 + 0.36) \n",
    "     = 1 - 0.52 \n",
    "     = 0.48\n",
    "```\n",
    "This indicates some **impurity** in the node since it contains elements from both classes.\n",
    "\n",
    "#### **Example 3: Completely Impure Node**\n",
    "Suppose the node contains 10 instances evenly distributed between `Class 0` and `Class 1` (5 instances each). The proportions are:\n",
    "- `p_0 = 5/10 = 0.5`\n",
    "- `p_1 = 5/10 = 0.5`\n",
    "\n",
    "The Gini Index would be:\n",
    "\n",
    "``` \n",
    "Gini = 1 - (0.5^2 + 0.5^2) \n",
    "     = 1 - (0.25 + 0.25) \n",
    "     = 1 - 0.5 \n",
    "     = 0.5\n",
    "```\n",
    "This indicates **maximum impurity** for a binary classification problem.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gini Index in Decision Trees**\n",
    "\n",
    "In decision trees (e.g., **CART** - Classification and Regression Trees), the Gini Index is used as a criterion to determine the best split. The algorithm chooses the feature and threshold that results in the **lowest weighted average Gini Index** for the child nodes after the split.\n",
    "\n",
    "1. **Pre-Split Gini Index**: The Gini Index of the node before splitting.\n",
    "2. **Post-Split Gini Index**: The Gini Index of the child nodes after the split. The algorithm calculates the weighted average of these values based on the size of the child nodes.\n",
    "\n",
    "The split that results in the greatest reduction in Gini Index (the highest reduction in impurity) is selected.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Gini Index**\n",
    "\n",
    "- **Computationally efficient**: Gini Index is faster to compute than entropy, which involves logarithmic calculations.\n",
    "- **Effective for Classification**: It performs well in decision tree algorithms for classification tasks, creating splits that help separate the classes effectively.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "The **Gini Index** is a measure of node impurity used to guide the decision-making process in decision trees. By minimizing Gini, the tree becomes better at creating pure branches, which improve the model's predictive power. The Gini Index is popular in the CART algorithm because of its simplicity and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
