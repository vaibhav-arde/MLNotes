{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Ensemble Techniques in Machine Learning**\n",
        "\n",
        "Ensemble techniques in machine learning are methods that combine multiple models to improve the overall performance of predictive models. The idea is that by combining several models, especially those that are weak learners (models with slightly better accuracy than random chance), you can create a stronger predictive model. Ensembles help reduce the variance, bias, or improve the accuracy of machine learning models.\n",
        "\n",
        "### Types of Ensemble Techniques\n",
        "\n",
        "There are primarily three types of ensemble methods:\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating)**\n",
        "2. **Boosting**\n",
        "3. **Stacking**\n",
        "\n",
        "Let’s go into detail about each type along with examples.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Bagging (Bootstrap Aggregating)**\n",
        "\n",
        "**Objective:** Reduce variance and avoid overfitting by creating multiple instances of the same model on different subsets of the data.\n",
        "\n",
        "- **How it works:** \n",
        "  - Bagging involves training multiple models (often of the same type) on different random subsets of the training data, generated through bootstrapping (sampling with replacement).\n",
        "  - After training, the predictions of all the models are combined, typically by averaging for regression tasks or by majority voting for classification tasks.\n",
        "  \n",
        "- **Example:** \n",
        "  - **Random Forest** is the most well-known example of the bagging technique. It is a collection of decision trees, where each tree is trained on a random subset of the data and features. The trees' predictions are aggregated by majority voting (classification) or averaging (regression).\n",
        "\n",
        "- **Advantages:**\n",
        "  - Reduces variance and prevents overfitting in models prone to it, like decision trees.\n",
        "  - Works well when individual models have high variance.\n",
        "\n",
        "- **Disadvantages:**\n",
        "  - Can lead to loss of interpretability.\n",
        "  - Computationally expensive due to the training of multiple models.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Boosting**\n",
        "\n",
        "**Objective:** Reduce bias by building models sequentially, where each subsequent model tries to correct the errors of the previous one.\n",
        "\n",
        "- **How it works:** \n",
        "  - Boosting algorithms train models sequentially. Each new model is trained to correct the errors made by the previous models, typically by giving more weight to the data points that were misclassified or poorly predicted.\n",
        "  - The final prediction is a weighted combination of all models.\n",
        "\n",
        "- **Types of Boosting:**\n",
        "  - **AdaBoost (Adaptive Boosting):**\n",
        "    - **Process:** Models are trained sequentially. The first model is trained on the original data, then weights are assigned to all data points. Misclassified data points are given more weight, and the next model focuses more on them. The process repeats, and the final prediction is a weighted sum of all models.\n",
        "    - **Example:** An AdaBoost classifier with decision stumps (simple decision trees with one node) as weak learners. The decision stumps are combined into a final strong classifier.\n",
        "  \n",
        "  - **Gradient Boosting:**\n",
        "    - **Process:** Models are trained sequentially. However, instead of adjusting weights, each new model is trained on the residual errors (differences between the actual and predicted values) of the previous model. It is a gradient descent approach to minimize a loss function.\n",
        "    - **Example:** **XGBoost**, **LightGBM**, and **CatBoost** are popular gradient boosting algorithms that are highly optimized for speed and performance.\n",
        "  \n",
        "- **Advantages:**\n",
        "  - Often results in state-of-the-art performance for many machine learning tasks.\n",
        "  - Boosting works well for complex data and can handle both regression and classification tasks effectively.\n",
        "\n",
        "- **Disadvantages:**\n",
        "  - Prone to overfitting if not properly regularized.\n",
        "  - Computationally expensive due to the sequential training of models.\n",
        "  \n",
        "---\n",
        "\n",
        "### 3. **Stacking**\n",
        "\n",
        "**Objective:** Improve predictive accuracy by combining different models (base models and meta-models) in a two-layered structure.\n",
        "\n",
        "- **How it works:**\n",
        "  - Stacking involves training multiple different models (referred to as base models) on the same dataset.\n",
        "  - A meta-model (or second-level model) is trained to make the final predictions based on the predictions of the base models.\n",
        "  - For example, you could have three different algorithms as base models (e.g., a decision tree, a logistic regression, and a neural network). The meta-model would then combine their predictions in some way (e.g., using linear regression, logistic regression, etc.) to make a final prediction.\n",
        "\n",
        "- **Example:** \n",
        "  - Suppose you have three base models: a Decision Tree, a Support Vector Machine (SVM), and a KNN classifier. You use their predictions as features to train a meta-model like a Logistic Regression, which makes the final prediction.\n",
        "  \n",
        "- **Advantages:**\n",
        "  - Can provide better predictive performance than any single model.\n",
        "  - Effective when base models are diverse (e.g., different algorithms or different subsets of the data).\n",
        "\n",
        "- **Disadvantages:**\n",
        "  - Complex to implement and interpret.\n",
        "  - Can be computationally expensive and prone to overfitting if not carefully managed.\n",
        "\n",
        "---\n",
        "\n",
        "### Example of Ensemble Technique: Random Forest (Bagging)\n",
        "\n",
        "Random Forest uses the bagging approach to combine multiple decision trees. The process looks like this:\n",
        "\n",
        "1. Take multiple bootstrap samples from the dataset (with replacement).\n",
        "2. Train a decision tree on each sample.\n",
        "3. At each split in the decision tree, a random subset of features is chosen, and the best feature is used for the split.\n",
        "4. Combine the trees’ predictions by majority voting for classification tasks or averaging for regression tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Ensemble Techniques Work?\n",
        "\n",
        "Ensemble methods leverage the power of combining different models (or the same model on different data subsets) to reduce the weaknesses of individual models. The models compensate for each other's weaknesses, leading to improved performance. It’s like a team of experts providing different perspectives to solve a problem—where individual mistakes are balanced out by the overall wisdom of the group.\n",
        "\n",
        "### Applications of Ensemble Methods:\n",
        "\n",
        "- **Fraud Detection:** Combining models to accurately identify fraudulent transactions.\n",
        "- **Stock Market Prediction:** Using ensembles to combine predictions from multiple models for better forecasting.\n",
        "- **Medical Diagnosis:** Creating robust models by combining weak learners to reduce error rates in critical medical predictions.\n",
        "- **Recommendation Systems:** Using ensemble methods to combine different algorithms to improve recommendation quality.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ensemble techniques are one of the most effective ways to improve the accuracy and robustness of machine learning models. They are widely used in both classification and regression tasks across various domains. By combining multiple models, ensemble methods help mitigate the limitations of individual models and provide more reliable and accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Ensemble techniques offer several advantages, but they also come with certain drawbacks. Below is a breakdown of the pros and cons of ensemble methods in machine learning.\n",
        "\n",
        "### **Pros of Ensemble Techniques**\n",
        "\n",
        "1. **Improved Accuracy**:\n",
        "   - By combining multiple models, ensemble methods often provide better predictive performance compared to individual models. This is because they capitalize on the strengths of each model while minimizing individual weaknesses.\n",
        "   \n",
        "2. **Reduced Overfitting (Variance)**:\n",
        "   - Methods like bagging (e.g., Random Forest) help reduce overfitting by training multiple models on different subsets of the data, averaging the results to smooth out variance.\n",
        "\n",
        "3. **Increased Robustness**:\n",
        "   - Ensemble methods provide more reliable predictions by balancing out errors from individual models. For example, if one model misclassifies an instance, the other models may correct that mistake.\n",
        "\n",
        "4. **Flexibility**:\n",
        "   - Ensemble techniques can combine models of different types (e.g., decision trees, neural networks, SVMs), making them highly adaptable to different types of data and problems.\n",
        "\n",
        "5. **Handling Complex Data**:\n",
        "   - In many cases, ensemble methods can better handle complex datasets with a large number of features or high variability in the data (e.g., non-linear patterns, noisy data).\n",
        "\n",
        "6. **Applicability in Many Scenarios**:\n",
        "   - Ensemble methods can be used for both regression and classification problems. They are also highly applicable to real-world scenarios such as fraud detection, medical diagnosis, stock prediction, etc.\n",
        "\n",
        "7. **Stability**:\n",
        "   - By averaging out predictions, ensemble methods often provide more stable and consistent results over time, reducing the impact of outliers or noise in the data.\n",
        "\n",
        "### **Cons of Ensemble Techniques**\n",
        "\n",
        "1. **Increased Computational Cost**:\n",
        "   - Training multiple models simultaneously or sequentially (as in boosting) requires significant computational power and memory. Ensemble methods like Random Forest or Gradient Boosting may require more resources than individual models.\n",
        "\n",
        "2. **Interpretability**:\n",
        "   - While individual models like decision trees are often easy to interpret, ensembles of models (especially in boosting or stacking) are much harder to explain. This lack of transparency can be problematic in applications where model interpretability is important (e.g., healthcare).\n",
        "\n",
        "3. **Longer Training Times**:\n",
        "   - Because multiple models need to be trained, ensemble methods can take much longer to train compared to single models. Techniques like boosting, where models are trained sequentially, can especially suffer from long training times.\n",
        "\n",
        "4. **Risk of Overfitting (in Boosting)**:\n",
        "   - Boosting techniques are prone to overfitting if not properly regularized. By continuously focusing on correcting the errors of previous models, they can become overly complex and fit noise in the training data.\n",
        "\n",
        "5. **Complexity in Implementation**:\n",
        "   - Ensemble methods, especially stacking or custom ensembles, can be complex to implement. They often require careful tuning of hyperparameters and a deep understanding of the models being combined.\n",
        "\n",
        "6. **Diminishing Returns**:\n",
        "   - After a certain point, adding more models to the ensemble may not significantly improve performance. There can be diminishing returns, where increasing complexity adds computational cost but no tangible improvement in accuracy.\n",
        "\n",
        "7. **Requires More Data**:\n",
        "   - In some cases, ensemble methods require more data to be effective. For example, bagging techniques like Random Forest work best when there’s a large dataset to generate varied training subsets.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "**Pros**:\n",
        "- Better accuracy and generalization.\n",
        "- Reduced overfitting (variance).\n",
        "- Robustness to noise and outliers.\n",
        "- Flexibility across models and problem types.\n",
        "- Suitable for complex data.\n",
        "\n",
        "**Cons**:\n",
        "- High computational cost and longer training times.\n",
        "- Harder to interpret.\n",
        "- Risk of overfitting (in boosting).\n",
        "- Complex to implement and maintain.\n",
        "- Possible diminishing returns with more models.\n",
        "\n",
        "In summary, ensemble methods are powerful tools in machine learning that improve model performance at the cost of increased complexity and computational demand. Choosing to use them depends on the specific problem, the size of the dataset, the need for interpretability, and available computational resources.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Bagging Implementation**\n",
        "Bagging (Bootstrap Aggregating) is an ensemble learning technique that aims to improve the accuracy and robustness of models, particularly decision trees. It does so by training multiple instances of a model on different subsets of the data (bootstrapped samples) and then averaging their predictions (for regression) or using majority voting (for classification).\n",
        "\n",
        "Here, I will demonstrate how to implement Bagging using Python and Scikit-learn. This example uses the `BaggingClassifier` and `BaggingRegressor` provided by Scikit-learn.\n",
        "\n",
        "### Bagging Implementation for Classification\n",
        "\n",
        "In this example, we will use the Bagging technique with decision trees to classify data.\n",
        "\n",
        "```python\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Bagging Classifier with Decision Tree as the base estimator\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,  # Number of trees\n",
        "    random_state=42,\n",
        "    bootstrap=True  # Whether to use bootstrapped datasets\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "### Bagging Implementation for Regression\n",
        "\n",
        "In this example, we will use Bagging with decision trees for regression purposes.\n",
        "\n",
        "```python\n",
        "# Import libraries\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import load_boston  # Boston dataset deprecated, use fetch_california_housing or another dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset (use California Housing or your dataset)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "data = fetch_california_housing()\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Bagging Regressor with Decision Tree as the base estimator\n",
        "bagging_reg = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,  # Number of trees\n",
        "    random_state=42,\n",
        "    bootstrap=True  # Whether to use bootstrapped datasets\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor Mean Squared Error: {mse:.2f}\")\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "- **Base Estimator**: The individual model used within the bagging ensemble, which is a decision tree in this case. You can replace it with any other model (like `SVC`, `LogisticRegression`, etc.).\n",
        "- **n_estimators**: The number of base estimators (e.g., decision trees) to be used.\n",
        "- **bootstrap**: Whether to use bootstrapping (sampling with replacement). If `False`, all samples are used for training each base model.\n",
        "- **random_state**: This ensures reproducibility.\n",
        "\n",
        "### How Bagging Works\n",
        "\n",
        "1. **Bootstrapping**: Random subsets of the original training data are created, with replacement, for training individual models. This means that some data points will appear multiple times in the training set, while others may be left out.\n",
        "2. **Training**: Multiple models (here, decision trees) are trained on different bootstrapped datasets.\n",
        "3. **Aggregation**: For classification, the final prediction is made by majority voting. For regression, the final prediction is the average of predictions from individual models.\n",
        "\n",
        "### Pros of Bagging\n",
        "- **Reduces Overfitting**: By averaging predictions from many models, bagging smooths out predictions and reduces overfitting.\n",
        "- **Improved Accuracy**: By combining multiple models, bagging can achieve higher accuracy than a single model, particularly for unstable models like decision trees.\n",
        "\n",
        "### Cons of Bagging\n",
        "- **Increased Complexity**: Training and maintaining multiple models requires more computational resources.\n",
        "- **Interpretability**: The ensemble model can be harder to interpret than individual models, particularly when using decision trees which are otherwise easy to understand.\n",
        "\n",
        "This approach is widely used in classification and regression problems to improve model performance and robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wjND7KRIiF30"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mzeYBmAxqUKA"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4b2_5orqdPf",
        "outputId": "25609369-b509-4feb-b988-cd389277261d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "         1.189e-01],\n",
              "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "         8.902e-02],\n",
              "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "         8.758e-02],\n",
              "        ...,\n",
              "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "         7.820e-02],\n",
              "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "         1.240e-01],\n",
              "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "         7.039e-02]]),\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " 'frame': None,\n",
              " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
              " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 569\\n\\n:Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n:Attribute Information:\\n    - radius (mean of distances from center to points on the perimeter)\\n    - texture (standard deviation of gray-scale values)\\n    - perimeter\\n    - area\\n    - smoothness (local variation in radius lengths)\\n    - compactness (perimeter^2 / area - 1.0)\\n    - concavity (severity of concave portions of the contour)\\n    - concave points (number of concave portions of the contour)\\n    - symmetry\\n    - fractal dimension (\"coastline approximation\" - 1)\\n\\n    The mean, standard error, and \"worst\" or largest (mean of the three\\n    worst/largest values) of these features were computed for each image,\\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n    10 is Radius SE, field 20 is Worst Radius.\\n\\n    - class:\\n            - WDBC-Malignant\\n            - WDBC-Benign\\n\\n:Summary Statistics:\\n\\n===================================== ====== ======\\n                                        Min    Max\\n===================================== ====== ======\\nradius (mean):                        6.981  28.11\\ntexture (mean):                       9.71   39.28\\nperimeter (mean):                     43.79  188.5\\narea (mean):                          143.5  2501.0\\nsmoothness (mean):                    0.053  0.163\\ncompactness (mean):                   0.019  0.345\\nconcavity (mean):                     0.0    0.427\\nconcave points (mean):                0.0    0.201\\nsymmetry (mean):                      0.106  0.304\\nfractal dimension (mean):             0.05   0.097\\nradius (standard error):              0.112  2.873\\ntexture (standard error):             0.36   4.885\\nperimeter (standard error):           0.757  21.98\\narea (standard error):                6.802  542.2\\nsmoothness (standard error):          0.002  0.031\\ncompactness (standard error):         0.002  0.135\\nconcavity (standard error):           0.0    0.396\\nconcave points (standard error):      0.0    0.053\\nsymmetry (standard error):            0.008  0.079\\nfractal dimension (standard error):   0.001  0.03\\nradius (worst):                       7.93   36.04\\ntexture (worst):                      12.02  49.54\\nperimeter (worst):                    50.41  251.2\\narea (worst):                         185.2  4254.0\\nsmoothness (worst):                   0.071  0.223\\ncompactness (worst):                  0.027  1.058\\nconcavity (worst):                    0.0    1.252\\nconcave points (worst):               0.0    0.291\\nsymmetry (worst):                     0.156  0.664\\nfractal dimension (worst):            0.055  0.208\\n===================================== ====== ======\\n\\n:Missing Attribute Values: None\\n\\n:Class Distribution: 212 - Malignant, 357 - Benign\\n\\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n:Donor: Nick Street\\n\\n:Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. dropdown:: References\\n\\n  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\\n    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\\n    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n    San Jose, CA, 1993.\\n  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\\n    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\\n    July-August 1995.\\n  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\\n    163-171.\\n',\n",
              " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "        'smoothness error', 'compactness error', 'concavity error',\n",
              "        'concave points error', 'symmetry error',\n",
              "        'fractal dimension error', 'worst radius', 'worst texture',\n",
              "        'worst perimeter', 'worst area', 'worst smoothness',\n",
              "        'worst compactness', 'worst concavity', 'worst concave points',\n",
              "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
              " 'filename': 'breast_cancer.csv',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7FRPkmUzqgIo"
      },
      "outputs": [],
      "source": [
        "## Split the data into input and target feature\n",
        "X = data.data\n",
        "y = data.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AqHkuSzmqyco"
      },
      "outputs": [],
      "source": [
        "## Split the data into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UE6FY-VrDN4",
        "outputId": "18c35193-6879-4da7-e206-7e6b0f34db53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.916083916083916"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Create a model of KNeighoursClassifer\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "knn.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGThBEBCrZ6x",
        "outputId": "67a507b0-df0c-4b59-f1a0-f8c5c94b962f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = knn.predict(X_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCR6q63crgLH",
        "outputId": "9340857a-ac6c-4f33-f43f-c22c14b796aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.916083916083916"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Create a model of BaggingClassifier\n",
        "bag_knn = BaggingClassifier(knn, n_estimators=10)\n",
        "bag_knn.fit(X_train, y_train)\n",
        "bag_knn.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VyqqpCSr7ug",
        "outputId": "d61a4bca-be66-4ab4-b5f2-7ed73ec42b0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred_bag_knn = bag_knn.predict(X_test)\n",
        "y_pred_bag_knn"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
