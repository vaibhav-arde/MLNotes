{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, instance-based, non-parametric machine learning algorithm used for both classification and regression tasks. The basic idea is to predict the label or value of a new data point based on the labels or values of the closest data points (neighbors) in the feature space.\n",
    "\n",
    "### **How KNN Works**\n",
    "KNN works by following these steps:\n",
    "1. **Select the number of neighbors (K):** Choose a value of K, which is the number of nearest neighbors to consider for making a prediction.\n",
    "2. **Compute distances:** Calculate the distance between the new data point and all other points in the dataset using a distance metric (typically Euclidean distance, though other metrics such as Manhattan, Minkowski, etc., can be used).\n",
    "3. **Identify neighbors:** Find the K points that are closest to the new data point based on the computed distances.\n",
    "4. **Make a prediction:**\n",
    "   - **Classification:** Assign the most common class (mode) among the K nearest neighbors to the new data point.\n",
    "   - **Regression:** Assign the average (mean) value of the K nearest neighbors to the new data point.\n",
    "\n",
    "### **KNN for Classification: Example**\n",
    "\n",
    "Let’s consider an example where we classify whether a flower is a **setosa** or **versicolor** based on its petal width and length.\n",
    "\n",
    "- **Dataset:**\n",
    "   - **Features:** Petal width and length.\n",
    "   - **Labels:** `Setosa` and `Versicolor`.\n",
    "  \n",
    "- **New Data Point:** Suppose we have a new flower with petal width = 1.5 and length = 4.5, and we want to classify whether it is `Setosa` or `Versicolor`.\n",
    "\n",
    "**Steps:**\n",
    "1. **Choose K:** Let’s select \\( K = 3 \\).\n",
    "2. **Calculate Distances:** Calculate the Euclidean distance between the new flower and all the other flowers in the dataset:\n",
    "   ```python\n",
    "   d(x, x_i) = sqrt(sum((x_j - x_{i,j})^2 for j in range(n)))\n",
    "   ```\n",
    "3. **Find the 3 Nearest Neighbors:** Based on the calculated distances, identify the 3 nearest flowers to the new data point.\n",
    "4. **Classify:** Take a majority vote among the 3 nearest neighbors. If 2 out of the 3 neighbors are classified as `Setosa`, and 1 is classified as `Versicolor`, the new flower would be classified as `Setosa`.\n",
    "\n",
    "### **KNN for Regression: Example**\n",
    "\n",
    "Now, let’s consider a regression problem where we want to predict the house price based on features like the number of rooms, lot size, and proximity to a city center.\n",
    "\n",
    "- **Dataset:**\n",
    "   - **Features:** Number of rooms, lot size, distance from city center.\n",
    "   - **Labels:** House prices.\n",
    "\n",
    "- **New Data Point:** Suppose we have a new house with 3 rooms, a lot size of 500 sq meters, and it is 10 km from the city center. We want to predict its price.\n",
    "\n",
    "**Steps:**\n",
    "1. **Choose K:** Let’s select \\( K = 3 \\).\n",
    "2. **Calculate Distances:** Compute the Euclidean distance between the new house and all the other houses in the dataset based on their features.\n",
    "3. **Find the 3 Nearest Neighbors:** Identify the 3 houses that are closest to the new house.\n",
    "4. **Predict the Price:** Take the average of the prices of the 3 nearest houses. Suppose the prices of the nearest houses are $200,000, $220,000, and $210,000. Then the predicted price would be:\n",
    "   ```python\n",
    "   Predicted Price = (200000 + 220000 + 210000) / 3 = 210000\n",
    "   ```\n",
    "\n",
    "### **Key Points to Consider**\n",
    "\n",
    "1. **Distance Metric:** The choice of distance metric impacts KNN performance. Euclidean distance is the most common, but for categorical data, Hamming distance might be used, and for Manhattan distance, L1 norm can be used.\n",
    "\n",
    "2. **Value of K:**\n",
    "   - Small values of K (e.g., \\( K = 1 \\)) make the model sensitive to noise in the dataset but provide more flexible decision boundaries.\n",
    "   - Large values of K smooth out predictions but may blur the distinctions between different classes.\n",
    "\n",
    "3. **Scaling/Normalization:** Since KNN relies on distance calculations, it’s important to scale or normalize the feature values to avoid one feature dominating the distance calculation. For instance, features with larger ranges can distort the distances unless the data is scaled.\n",
    "\n",
    "4. **Lazy Learner:** KNN is a **lazy learning algorithm**, meaning it doesn’t learn a model during the training phase. All computation is deferred until prediction, which can lead to high computational cost when the dataset is large.\n",
    "\n",
    "### **Advantages of KNN**\n",
    "- **Simplicity:** Easy to implement and understand.\n",
    "- **No Training Phase:** No explicit model training is required, making KNN suitable for dynamic datasets where data frequently changes.\n",
    "- **Versatility:** Can be used for both classification and regression tasks.\n",
    "\n",
    "### **Disadvantages of KNN**\n",
    "- **Computationally Expensive:** During prediction, KNN requires calculating the distance between the query point and all other points in the dataset, which can be slow for large datasets.\n",
    "- **Memory Intensive:** KNN stores all training data, which can consume a lot of memory.\n",
    "- **Sensitive to Irrelevant Features:** If irrelevant features are included, they can distort the distance metric and lead to poor performance. Therefore, feature selection is crucial.\n",
    "- **Imbalanced Datasets:** KNN struggles with imbalanced datasets where one class dominates, as the majority class will be more likely to be chosen.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "KNN is a versatile algorithm that can be used for both classification and regression. Its performance heavily depends on the choice of K, distance metric, and scaling of data. It is best suited for smaller datasets, as it requires a lot of computational resources for large datasets. However, its simplicity and effectiveness make it a popular choice for many applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
