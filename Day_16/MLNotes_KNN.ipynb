{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, instance-based, non-parametric machine learning algorithm used for both classification and regression tasks. The basic idea is to predict the label or value of a new data point based on the labels or values of the closest data points (neighbors) in the feature space.\n",
    "\n",
    "### **How KNN Works**\n",
    "KNN works by following these steps:\n",
    "1. **Select the number of neighbors (K):** Choose a value of K, which is the number of nearest neighbors to consider for making a prediction.\n",
    "2. **Compute distances:** Calculate the distance between the new data point and all other points in the dataset using a distance metric (typically Euclidean distance, though other metrics such as Manhattan, Minkowski, etc., can be used).\n",
    "3. **Identify neighbors:** Find the K points that are closest to the new data point based on the computed distances.\n",
    "4. **Make a prediction:**\n",
    "   - **Classification:** Assign the most common class (mode) among the K nearest neighbors to the new data point.\n",
    "   - **Regression:** Assign the average (mean) value of the K nearest neighbors to the new data point.\n",
    "\n",
    "### **KNN for Classification: Example**\n",
    "\n",
    "Let’s consider an example where we classify whether a flower is a **setosa** or **versicolor** based on its petal width and length.\n",
    "\n",
    "- **Dataset:**\n",
    "   - **Features:** Petal width and length.\n",
    "   - **Labels:** `Setosa` and `Versicolor`.\n",
    "  \n",
    "- **New Data Point:** Suppose we have a new flower with petal width = 1.5 and length = 4.5, and we want to classify whether it is `Setosa` or `Versicolor`.\n",
    "\n",
    "**Steps:**\n",
    "1. **Choose K:** Let’s select \\( K = 3 \\).\n",
    "2. **Calculate Distances:** Calculate the Euclidean distance between the new flower and all the other flowers in the dataset:\n",
    "   ```python\n",
    "   d(x, x_i) = sqrt(sum((x_j - x_{i,j})^2 for j in range(n)))\n",
    "   ```\n",
    "3. **Find the 3 Nearest Neighbors:** Based on the calculated distances, identify the 3 nearest flowers to the new data point.\n",
    "4. **Classify:** Take a majority vote among the 3 nearest neighbors. If 2 out of the 3 neighbors are classified as `Setosa`, and 1 is classified as `Versicolor`, the new flower would be classified as `Setosa`.\n",
    "\n",
    "### **KNN for Regression: Example**\n",
    "\n",
    "Now, let’s consider a regression problem where we want to predict the house price based on features like the number of rooms, lot size, and proximity to a city center.\n",
    "\n",
    "- **Dataset:**\n",
    "   - **Features:** Number of rooms, lot size, distance from city center.\n",
    "   - **Labels:** House prices.\n",
    "\n",
    "- **New Data Point:** Suppose we have a new house with 3 rooms, a lot size of 500 sq meters, and it is 10 km from the city center. We want to predict its price.\n",
    "\n",
    "**Steps:**\n",
    "1. **Choose K:** Let’s select \\( K = 3 \\).\n",
    "2. **Calculate Distances:** Compute the Euclidean distance between the new house and all the other houses in the dataset based on their features.\n",
    "3. **Find the 3 Nearest Neighbors:** Identify the 3 houses that are closest to the new house.\n",
    "4. **Predict the Price:** Take the average of the prices of the 3 nearest houses. Suppose the prices of the nearest houses are $200,000, $220,000, and $210,000. Then the predicted price would be:\n",
    "   ```python\n",
    "   Predicted Price = (200000 + 220000 + 210000) / 3 = 210000\n",
    "   ```\n",
    "\n",
    "### **Key Points to Consider**\n",
    "\n",
    "1. **Distance Metric:** The choice of distance metric impacts KNN performance. Euclidean distance is the most common, but for categorical data, Hamming distance might be used, and for Manhattan distance, L1 norm can be used.\n",
    "\n",
    "2. **Value of K:**\n",
    "   - Small values of K (e.g., \\( K = 1 \\)) make the model sensitive to noise in the dataset but provide more flexible decision boundaries.\n",
    "   - Large values of K smooth out predictions but may blur the distinctions between different classes.\n",
    "\n",
    "3. **Scaling/Normalization:** Since KNN relies on distance calculations, it’s important to scale or normalize the feature values to avoid one feature dominating the distance calculation. For instance, features with larger ranges can distort the distances unless the data is scaled.\n",
    "\n",
    "4. **Lazy Learner:** KNN is a **lazy learning algorithm**, meaning it doesn’t learn a model during the training phase. All computation is deferred until prediction, which can lead to high computational cost when the dataset is large.\n",
    "\n",
    "### **Advantages of KNN**\n",
    "- **Simplicity:** Easy to implement and understand.\n",
    "- **No Training Phase:** No explicit model training is required, making KNN suitable for dynamic datasets where data frequently changes.\n",
    "- **Versatility:** Can be used for both classification and regression tasks.\n",
    "\n",
    "### **Disadvantages of KNN**\n",
    "- **Computationally Expensive:** During prediction, KNN requires calculating the distance between the query point and all other points in the dataset, which can be slow for large datasets.\n",
    "- **Memory Intensive:** KNN stores all training data, which can consume a lot of memory.\n",
    "- **Sensitive to Irrelevant Features:** If irrelevant features are included, they can distort the distance metric and lead to poor performance. Therefore, feature selection is crucial.\n",
    "- **Imbalanced Datasets:** KNN struggles with imbalanced datasets where one class dominates, as the majority class will be more likely to be chosen.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "KNN is a versatile algorithm that can be used for both classification and regression. Its performance heavily depends on the choice of K, distance metric, and scaling of data. It is best suited for smaller datasets, as it requires a lot of computational resources for large datasets. However, its simplicity and effectiveness make it a popular choice for many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Distance Metrics in KNN**\n",
    "\n",
    "The distance metric is a crucial aspect of KNN, as it determines how \"closeness\" between data points is calculated.\n",
    "\n",
    "#### **1. Euclidean Distance**\n",
    "This is the most commonly used distance metric in KNN, which calculates the straight-line distance between two points in a multidimensional space.\n",
    "\n",
    "- **Formula:**\n",
    "  ```\n",
    "  d(p, q) = sqrt(sum((p_i - q_i)^2 for i in range(n)))\n",
    "  ```\n",
    "  where \\( p \\) and \\( q \\) are the points (or vectors) with \\( n \\) features.\n",
    "\n",
    "#### **2. Manhattan Distance**\n",
    "Manhattan distance, also called L1 distance or city block distance, calculates the distance by summing the absolute differences of their coordinates.\n",
    "\n",
    "- **Formula:**\n",
    "  ```\n",
    "  d(p, q) = sum(|p_i - q_i| for i in range(n))\n",
    "  ```\n",
    "\n",
    "#### **3. Hamming Distance**\n",
    "Hamming distance is used for categorical variables and counts the number of positions where the corresponding elements differ.\n",
    "\n",
    "- **Formula:**\n",
    "  ```\n",
    "  d(p, q) = sum(1 if p_i != q_i else 0 for i in range(n))\n",
    "  ```\n",
    "\n",
    "#### **4. Minkowski Distance**\n",
    "Minkowski distance is a generalization of both Euclidean and Manhattan distances.\n",
    "\n",
    "- **Formula:**\n",
    "  ```\n",
    "  d(p, q) = (sum(|p_i - q_i|^r for i in range(n)))^(1/r)\n",
    "  ```\n",
    "\n",
    "For Euclidean distance, \\( r = 2 \\), and for Manhattan distance, \\( r = 1 \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Hyperparameters in KNN**\n",
    "\n",
    "#### **1. K (Number of Neighbors)**\n",
    "- **Role:** Determines how many neighbors are considered when making predictions.\n",
    "  \n",
    "- **Choosing K:** The optimal value of \\( K \\) can be determined using cross-validation. Often, a good starting point is \\( K = \\sqrt{n} \\), where \\( n \\) is the number of data points.\n",
    "\n",
    "#### **2. Distance Metric**\n",
    "- **Role:** Defines how the distance between points is measured. Common choices include Euclidean, Manhattan, and Hamming distances.\n",
    "\n",
    "#### **3. Weights**\n",
    "- **Role:** Assigns weights to the neighbors. You can use uniform weights, where each neighbor is weighted equally, or distance-based weights, where closer neighbors have more influence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Choosing the Value of K**\n",
    "\n",
    "1. **Cross-Validation:** Cross-validation is often used to select the best value of \\( K \\).\n",
    "  \n",
    "2. **Empirical Rule:** A commonly used rule of thumb is to set \\( K \\) to the square root of the number of samples:\n",
    "  ```\n",
    "  K = sqrt(n)\n",
    "  ```\n",
    "\n",
    "3. **Error Rates:** Larger values of \\( K \\) lead to smoother decision boundaries but increase bias (underfitting), while smaller values decrease bias but increase variance (overfitting).\n",
    "\n",
    "---\n",
    "### **Conclusion**\n",
    "\n",
    "KNN's performance is influenced by the choice of distance metric, the number of neighbors (K), and how the neighbors are weighted. Cross-validation is a robust way to select the optimal value for these hyperparameters, ensuring that the model performs well on unseen data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **KNN: Pros and Cons**\n",
    "\n",
    "#### **Pros of KNN**\n",
    "\n",
    "1. **No Strong Assumptions:**\n",
    "   - **Explanation:** Unlike other algorithms like Linear Regression or SVM, KNN doesn't make strong assumptions about the data. It is a non-parametric algorithm, meaning it doesn't assume an underlying probability distribution or model structure.\n",
    "   - **Benefit:** This makes KNN highly flexible and able to work well with data that doesn't fit into predefined assumptions, such as nonlinear data or complex decision boundaries.\n",
    "\n",
    "2. **Simple to Implement and Understand:**\n",
    "   - **Explanation:** KNN is conceptually simple and easy to understand. The idea of finding the closest points to make a decision based on the majority is intuitive.\n",
    "   - **Benefit:** Easy implementation in applications, even with minimal data preparation or preprocessing.\n",
    "\n",
    "3. **Adaptable to Classification and Regression:**\n",
    "   - **Explanation:** KNN can be used for both classification and regression tasks, making it versatile.\n",
    "   - **Benefit:** Useful in various domains and applications like pattern recognition and data mining.\n",
    "\n",
    "4. **Low Training Cost:**\n",
    "   - **Explanation:** As a lazy learner, KNN doesn’t involve any model training phase. All the computation happens at prediction time.\n",
    "   - **Benefit:** No expensive computations are required to build a model beforehand. You only need to store the data points.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Cons of KNN**\n",
    "\n",
    "1. **Lazy Learner:**\n",
    "   - **Explanation:** KNN is considered a lazy learner because it doesn't learn a model during the training phase. It simply memorizes the training data and makes predictions during the testing phase by calculating distances to the training data.\n",
    "   - **Drawback:** The prediction phase can be very slow, especially with large datasets, because KNN has to compute distances between a test point and every single training point. This leads to high computational cost during prediction, which makes it inefficient for real-time applications.\n",
    "\n",
    "2. **Choice of K (Optimal Value):**\n",
    "   - **Explanation:** The accuracy of KNN heavily depends on the choice of \\( K \\) (number of neighbors).\n",
    "   - **Drawback:** Too small a \\( K \\) leads to high variance (overfitting), while too large a \\( K \\) leads to high bias (underfitting). It is often necessary to use techniques like cross-validation to find the optimal value of \\( K \\).\n",
    "\n",
    "3. **High Memory Requirement:**\n",
    "   - **Explanation:** Since KNN needs to store all the training data, it requires a significant amount of memory, especially with large datasets.\n",
    "   - **Drawback:** This can make KNN impractical for memory-constrained systems or for handling very large datasets.\n",
    "\n",
    "4. **Sensitive to Outliers and Noisy Data:**\n",
    "   - **Explanation:** KNN is sensitive to the presence of noisy data points or outliers because they can skew the classification or regression results.\n",
    "   - **Drawback:** Preprocessing steps like outlier detection, noise removal, and scaling become crucial to improving KNN's performance.\n",
    "\n",
    "5. **Scaling and Normalization Required:**\n",
    "   - **Explanation:** KNN relies on distance measures, which means features with larger magnitudes will dominate the results. Therefore, data needs to be scaled or normalized before applying KNN.\n",
    "   - **Drawback:** Preprocessing can become an additional step in the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of KNN**\n",
    "\n",
    "1. **Recommendation Systems:**\n",
    "   - KNN is used in recommendation systems to find users or items that are similar to a given user or item. By calculating the distance between a user and others based on preferences, KNN can recommend items that similar users liked.\n",
    "\n",
    "2. **Image Recognition:**\n",
    "   - KNN is applied in image recognition tasks to classify images based on features extracted from the images. For example, handwritten digit recognition using KNN on image pixel values.\n",
    "\n",
    "3. **Medical Diagnosis:**\n",
    "   - In medical diagnosis, KNN can be used to classify patients based on the similarity of their symptoms or test results to other known cases, aiding in diagnosing diseases.\n",
    "\n",
    "4. **Anomaly Detection:**\n",
    "   - KNN is effective for anomaly detection, where the algorithm can detect outliers or unusual data points based on their distance from the majority of the data.\n",
    "\n",
    "5. **Pattern Recognition:**\n",
    "   - KNN is commonly used in pattern recognition tasks, such as text recognition and speech recognition, where it helps in categorizing text or audio data into pre-defined categories.\n",
    "\n",
    "---\n",
    "\n",
    "### **SMOTE (Synthetic Minority Over-sampling Technique)**\n",
    "\n",
    "SMOTE is a popular technique used to address imbalanced datasets, where one class significantly outnumbers the other(s). In such cases, KNN's performance can suffer because the majority class dominates the decision boundaries. SMOTE helps by synthetically generating new data points for the minority class.\n",
    "\n",
    "#### **How SMOTE Works:**\n",
    "\n",
    "1. **Identify Minority Class Samples:**\n",
    "   - SMOTE identifies data points from the minority class that are under-represented.\n",
    "\n",
    "2. **KNN on Minority Class:**\n",
    "   - For each minority class data point, SMOTE identifies its K-nearest neighbors from the same minority class using the KNN algorithm.\n",
    "\n",
    "3. **Create Synthetic Data:**\n",
    "   - SMOTE creates synthetic data points by interpolating between the original minority class data point and its neighbors. This is done by selecting a random point along the line connecting the two points.\n",
    "   \n",
    "   - **Formula for Generating Synthetic Data:**\n",
    "     ```\n",
    "     new_sample = original_sample + (neighbor_sample - original_sample) * random(0, 1)\n",
    "     ```\n",
    "\n",
    "4. **Add Synthetic Samples to Dataset:**\n",
    "   - The newly generated synthetic data points are then added to the dataset, increasing the representation of the minority class.\n",
    "\n",
    "#### **Advantages of SMOTE:**\n",
    "- Reduces bias in favor of the majority class by creating a balanced dataset.\n",
    "- Helps improve the performance of classifiers on imbalanced data.\n",
    "\n",
    "#### **Disadvantages of SMOTE:**\n",
    "- It can introduce noise by generating synthetic data points that might not follow the true distribution of the minority class.\n",
    "- If applied without careful tuning, it might lead to overfitting, as synthetic samples might be very similar to the original ones.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **KNN Pros:** Works without strong assumptions, simple implementation, adaptable to both classification and regression, and has a low training cost.\n",
    "- **KNN Cons:** Slow during prediction, sensitive to the choice of \\( K \\), requires high memory, sensitive to noise, and requires data scaling.\n",
    "- **Applications of KNN:** Widely used in recommendation systems, image recognition, medical diagnosis, anomaly detection, and pattern recognition.\n",
    "- **SMOTE:** Balances imbalanced datasets by generating synthetic minority class samples through interpolation between minority class neighbors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
